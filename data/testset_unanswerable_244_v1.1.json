{"0": {"paper_id": "paper_1", "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization", "question_id": 2, "question": "From what I know, BLEU score also aligned with human evaluations when it was first released. Is this still the case? How is BLEURT better in this sense?", "question_section": "Introduction", "question_trigger_sentence": "Automated metrics offer a promising compromise: learned models of human preference like BERTScore (Zhang et al., 2019), BLEURT (Sellam et al., 2020), summarization preferences (Wu et al., 2021) have significantly improved correlation with human judgment compared to earlier metrics (BLEU, METEOR, etc.), and are cheap to evaluate.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "This paper does not contain information on how BLEU and BLEURT are different, or if BLEU scores aligned with human evaluations when it was originally released.", "arxiv_id": "2210.01241", "s2orc_url": "https://www.semanticscholar.org/paper/912a39c2e0e4a35747531669cfa952d2c5627729", "arxiv_url": "https://arxiv.org/abs/2210.01241"}, "1": {"paper_id": "paper_1", "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization", "question_id": 4, "question": "What is Schedule Sampling? What is the cliff MDP problem?", "question_section": "Related Work", "question_trigger_sentence": "Algorithms such as Schedule Sampling (SS) (Bengio et al., 2015), Parallel SS (Duckworth et al., 2019), SS for Transformers (Mihaylova & Martins, 2019), Diffential SS (Goyal et al., 2017), LOLS (Lampouras & Vlachos, 2016; Chang et al., 2015), and SEARNN (Leblond et al., 2017), have been inspired by DAGGER (Ross et al., 2011) and SEARN (Daum\u00e9 et al., 2009). However these algorithms are known to suffer from the cliff MDP problem (Husz\u00e1r, 2015; Agarwal et al., 2019; Swamy et al., 2021).", "question_type": "shallow question", "evidential_info": [], "composition": "The current paper does not define the terms \"schedule sampling\" or the \"cliff MDP problem\".", "arxiv_id": "2210.01241", "s2orc_url": "https://www.semanticscholar.org/paper/912a39c2e0e4a35747531669cfa952d2c5627729", "arxiv_url": "https://arxiv.org/abs/2210.01241"}, "2": {"paper_id": "paper_1", "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization", "question_id": 6, "question": "What is the main difference between RL for LM and supervised finetuning for LMs? If you are doing finetuning with human preferences as loss, is this different to RL? Why?", "question_section": "Related Work", "question_trigger_sentence": "RL for NLP. RL has been used to improve models in machine translation (Wu et al., 2016; Kiegeland & Kreutzer, 2021), summarization (Stiennon et al., 2020; Paulus et al., 2017), dialogue (Li et al., 2016; Zhou et al., 2017; Jaques et al., 2020), image captioning (Rennie et al., 2017), question generation (Pang & He, 2021), text-games (Narasimhan et al., 2015; Hausknecht et al., 2020), and more (Ranzato et al., 2016; Snell et al., 2022). Lu et al. (2022) adapt reward-conditioned transformers (Chen et al., 2021) for several language generation tasks. RL has been the focus of efforts to align LMs with human preferences (Stiennon et al., 2020; Wu et al., 2021; Nakano et al., 2021; Ziegler et al., 2019), e.g., Ouyang et al. (2022) fine-tuned a 175B parameter language model with PPO Schulman et al. (2017) to align with models of human preference, but their non-public dataset doesn\u2019t enable comparison.", "question_type": "testing question", "evidential_info": [], "composition": "The authors in this paper do not explain how RL approaches with human feedback are structurally different from finetuning supervised models using the human preference as the loss.", "arxiv_id": "2210.01241", "s2orc_url": "https://www.semanticscholar.org/paper/912a39c2e0e4a35747531669cfa952d2c5627729", "arxiv_url": "https://arxiv.org/abs/2210.01241"}, "3": {"paper_id": "paper_1", "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization", "question_id": 7, "question": "How does Generalized Advantage Estimation work?", "question_section": "On-policy Actor-critic Algorithms", "question_trigger_sentence": "To increase training stability, advantage is appoximated using Generalized Advantage Estimation (Schulman et al., 2015).", "question_type": "Deep/complex question", "evidential_info": [], "composition": "This paper does not contain information on how generalized advantage estimation works.", "arxiv_id": "2210.01241", "s2orc_url": "https://www.semanticscholar.org/paper/912a39c2e0e4a35747531669cfa952d2c5627729", "arxiv_url": "https://arxiv.org/abs/2210.01241"}, "4": {"paper_id": "paper_10", "title": "Understanding Neural Networks Through Deep Visualization", "question_id": 20, "question": "Why did the authors use random search for selection of hyperparameters?", "question_section": "Visualizing via Regularized Optimization", "question_trigger_sentence": "While each of these regularization methods helps on its own, in combination they are even more effective. We found useful combinations via a random hyperparameter search\n", "question_type": "Deep/complex question", "evidential_info": [], "composition": "It is a common way of searching for hyperparameters in Ml problems, and you can decide according to your needs whether to use it or not.There are also some several ways like grid search where you can imagine you have different sets of parameters located inside the search space and you are navigating through them to find collection that give best results, or the heuristic way where you just initialize hyperparameters with or maybe without some domain knowledge or prior beliefs and receive the results according to such case.Authors might find this way better for comparisons between different responses, activations and results.", "arxiv_id": "1506.06579", "s2orc_url": "https://www.semanticscholar.org/paper/1b5a24639fa80056d1a17b15f6997d10e76cc731", "arxiv_url": "https://arxiv.org/abs/1506.06579"}, "5": {"paper_id": "paper_10", "title": "Understanding Neural Networks Through Deep Visualization", "question_id": 5, "question": "An architectural change of smaller convolutional filters led to state of the art performance on the ImageNet benchmark in 2013. What was the achieved accuracy?", "question_section": "Introduction", "question_trigger_sentence": "For example, the deconvolutional technique for visualizing the features learned by the hidden units of DNNs suggested an architectural change of smaller convolutional filters that led to state of the art performance on the ImageNet benchmark in 2013.", "question_type": "Testing question", "evidential_info": [], "composition": "the modified architecture outperforms the architecture of Krizhevsky et al. beating their single model result by 1.7% (test top-5) and by combining multiple models, they obtain a test error of 14.8%, an improvement of 1.6%.", "arxiv_id": "1506.06579", "s2orc_url": "https://www.semanticscholar.org/paper/1b5a24639fa80056d1a17b15f6997d10e76cc731", "arxiv_url": "https://arxiv.org/abs/1506.06579"}, "6": {"paper_id": "paper_10", "title": "Understanding Neural Networks Through Deep Visualization", "question_id": 9, "question": "What is the maximum layer size of the paper's pretrained DNN?", "question_section": "Visualizing Live Convnet Activations", "question_trigger_sentence": "Because this convnet contains only a single path from input to output, every layer is a bottleneck through which all information must pass en-route to a classification decision. The layer sizes are all small enough that any one layer can easily fit on a computer screen.", "question_type": "Testing question", "evidential_info": [], "composition": "Each layer in DL has a special treatment for the input to it through a bunch of learnable parameters which may be shared across different parts of the same input to that layer(as is the case in conv layers) or can be unique across all parts of its input (as is the case with FC layer) so if the asked question is about largest size in terms of parameters the answer would be the first fully connected layer fc6 as it has an input vector from the pooling layer of dim=9216 and the layer outputs a vector of 4096 so it has parameters of nearly 37.750M parameters inside.", "arxiv_id": "1506.06579", "s2orc_url": "https://www.semanticscholar.org/paper/1b5a24639fa80056d1a17b15f6997d10e76cc731", "arxiv_url": "https://arxiv.org/abs/1506.06579"}, "7": {"paper_id": "paper_101", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "question_id": 2, "question": "If we lack of datasets for training the encoder that produces vector embeddings, will ColBERT still show better performance (e.g., high recall) compared to term-based retrieval models?", "question_section": "4. 1 Methodology", "question_trigger_sentence": "As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The performance of ColBERT can not be answered in this paper when training the encoder have small dataset.", "arxiv_id": "2004.12832", "s2orc_url": "https://www.semanticscholar.org/paper/60b8ad6177230ad5402af409a6edb5af441baeb4", "arxiv_url": "https://arxiv.org/abs/2004.12832"}, "8": {"paper_id": "paper_101", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "question_id": 4, "question": "Given that the training dataset does not provide non-relevant passages explicitly, how did the authors create a training triple that also requires non-relevant passages?", "question_section": "4. 3 End-to-end Top-k Retrieval", "question_trigger_sentence": "In this section, we examine ColBERT\u2019s efficiency and effectiveness at re-ranking the top-k results extracted by a bag-of-words retrieval model, which is the most typical setting for testing and deploying neural ranking models. We begin with the MS MARCO dataset.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The training triple of non-relevant passages cannot be answered in this paper.", "arxiv_id": "2004.12832", "s2orc_url": "https://www.semanticscholar.org/paper/60b8ad6177230ad5402af409a6edb5af441baeb4", "arxiv_url": "https://arxiv.org/abs/2004.12832"}, "9": {"paper_id": "paper_102", "title": "BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models", "question_id": 4, "question": "How did you choose hyper-parameters for GenQ such as training steps, the number of queries for each document, etc?", "question_section": "5 Results and Analysis", "question_trigger_sentence": "7. Does domain adaptation help improve generalization of dense-retrievers? We evaluated GenQ, which further fine-tunes the TAS-B model on synthetic query data. It outperforms the TAS-B model on specialized domains like scientific publications, finance or StackExchange. On broader and more generic domains, like Wikipedia, it performs weaker than the original TAS-B model.", "question_type": "Shallow question", "evidential_info": [], "composition": "The choice of hyper parameters cannot be answered in this paper.", "arxiv_id": "2104.08663", "s2orc_url": "https://www.semanticscholar.org/paper/807600ef43073cd9c59d4208ee710e90cf14efa8", "arxiv_url": "https://arxiv.org/abs/2104.08663"}, "10": {"paper_id": "paper_102", "title": "BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models", "question_id": 5, "question": "If we use large-scale, high-capacity generator (e.g., GPT-3, T5-XL) and thus have better training datasets for effective domain adaptation, can we further increase performance of dense retrievers and finally outperform BM25?", "question_section": "6 Impact of Annotation Selection Bias", "question_trigger_sentence": "Models need to potentially compare a single query against millions of documents at inference, hence, a high computational speed for retrieving results in real-time is desired. Besides speed, index sizes are vital and are often stored entirely in memory.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The performance of dense retrievers cannot be answered in this paper.", "arxiv_id": "2104.08663", "s2orc_url": "https://www.semanticscholar.org/paper/807600ef43073cd9c59d4208ee710e90cf14efa8", "arxiv_url": "https://arxiv.org/abs/2104.08663"}, "11": {"paper_id": "paper_102", "title": "BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models", "question_id": 7, "question": "If we target retrievers for semantic matching tasks such as kNN classifier for classification tasks, would this further aggravate the annotation/evaluation challenge?", "question_section": "Conclusion", "question_trigger_sentence": "In order to study the impact of this particular type of bias, we conducted a study on the recent TREC-COVID dataset. TREC-COVID used a pooling method [38, 40] to reduce the impact of the aforementioned bias: The annotation set was constructed by using the search results from the various systems participating in the challenge. Table 4 shows the Hole@10 rate [73] for the tested systems, i.e., how many top-10 hits is each system retrieving that have not been seen by annotators.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "If we target retrievers for semantic matching tasks such as kNN classifier for classification tasks, it cannot be answered from this paper whether annotation/evaluation challenge will be aggregated or not", "arxiv_id": "2104.08663", "s2orc_url": "https://www.semanticscholar.org/paper/807600ef43073cd9c59d4208ee710e90cf14efa8", "arxiv_url": "https://arxiv.org/abs/2104.08663"}, "12": {"paper_id": "paper_104", "title": "Intent Contrastive Learning for Sequential Recommendation", "question_id": 6, "question": "How does CoSeRec is different from CL4SRec with regard to utilizing a multi-task training framework?", "question_section": "2.3.Contrastive Self-Supervised Learning", "question_trigger_sentence": "CL4SRec (Xie et al., 2020) and CoSeRec (Liu et al., 2021a) instead utilize a multi-task training framework with a contrastive objective to enhance user representations. ", "question_type": "Testing question", "evidential_info": [], "composition": "Authors do not discuss their difference.", "arxiv_id": "2202.02519", "s2orc_url": "https://www.semanticscholar.org/paper/a4c269e1b7b8ffc0e0c6a9362ea40cc105a53b21", "arxiv_url": "https://arxiv.org/abs/2202.02519"}, "13": {"paper_id": "paper_11", "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and<0.5MB model size", "question_id": 6, "question": "What is an \"ad-hoc\" layer?", "question_section": "RELATED WORK", "question_trigger_sentence": "Many such modules are then combined, perhaps with additional ad-hoc layers, to form a complete network. We use the term CNN microarchitecture to refer to the particular organization and dimensions of the individual modules.", "question_type": "Shallow question", "evidential_info": [], "composition": "This term is beyond the scope of main discussion of the paper. It generally means to add layers arbitrarily to reach desired performance, but it is not answerable through the paper.", "arxiv_id": "1602.07360", "s2orc_url": "https://www.semanticscholar.org/paper/592d2e65489f23ebd993dbdc0c84eda9ac8aadbe", "arxiv_url": "https://arxiv.org/abs/1602.07360"}, "14": {"paper_id": "paper_113", "title": "YOLOv3: An Incremental Improvement", "question_id": 1, "question": "In YoloV3 new DarkNet-53 based backbone, what might be the advantage of using convolution layer with strides to down sample feature map instead of using a pooling layer?", "question_section": "Predictions Across Scales", "question_trigger_sentence": "We still train on full images with no hard negative mining or any of that stuff. We use multi-scale training, lots of data augmentation, batch normalization, all the standard stuff. We use the Darknet neural network framework for training and testing", "question_type": "Deep/complex question", "evidential_info": [], "composition": "This question is unique as I am not really sure why the reader came up with as the authors do not mention the details, as specified in the continuation. I would say the answer to this question is as simple as \"It just worked better\". However, I didn't find anywhere in the text that the authors mention either \"strides\" or \"pooling layers\". On the other hand, I found that the feature maps were only upsampled, and not downsampled. Therefore, I don't know how the reader came up with the question.", "arxiv_id": "1804.02767", "s2orc_url": "https://www.semanticscholar.org/paper/ebc96892b9bcbf007be9a1d7844e4b09fde9d961", "arxiv_url": "https://arxiv.org/abs/1804.02767"}, "15": {"paper_id": "paper_113", "title": "YOLOv3: An Incremental Improvement", "question_id": 2, "question": "How many trainable parameters does the yolov3 network have?", "question_section": "Bounding Box Prediction", "question_trigger_sentence": "YOLOv3 predicts boxes at 3 different scales. Our system extracts features from those scales using a similar concept to feature pyramid networks", "question_type": "Testing question", "evidential_info": [], "composition": "The question is unanswerable based on the paper. But I've found the answer on this GitHub issue (https://github.com/pjreddie/darknet/issues/1829): 65,252,682 parameters.", "arxiv_id": "1804.02767", "s2orc_url": "https://www.semanticscholar.org/paper/ebc96892b9bcbf007be9a1d7844e4b09fde9d961", "arxiv_url": "https://arxiv.org/abs/1804.02767"}, "16": {"paper_id": "paper_113", "title": "YOLOv3: An Incremental Improvement", "question_id": 17, "question": "How YoloV3 architecture is different from SSD variants?", "question_section": "Feature Extractor", "question_trigger_sentence": "YOLOv3 is a good detector. It\u2019s fast, it\u2019s accurate. It\u2019s not as great on the COCO average AP between .5 and .95 IOU metric. But it\u2019s very good on the old detection metric of .5 IOU.\n\nWhy did we switch metrics anyway? The original COCO paper just has this cryptic sentence: \u201cA full discussion of evaluation metrics will be added once the evaluation server is complete\u201d. Russakovsky et al report that that humans have a hard time distinguishing an IOU of .3 from .5! \u201cTraining humans to visually inspect a bounding box with IOU of 0.3 and distinguish it from one with IOU 0.5 is surprisingly difficult.\u201d [18] If humans have a hard time telling the difference, how much does it matter?", "question_type": "Testing question", "evidential_info": [], "composition": "The question is not answerable based on the materials from the paper, but in general, the difference between YOLOv3 and SSD architecture is that YOLOv3, i.e., DarkNet-53 makes particular use of residual connections.", "arxiv_id": "1804.02767", "s2orc_url": "https://www.semanticscholar.org/paper/ebc96892b9bcbf007be9a1d7844e4b09fde9d961", "arxiv_url": "https://arxiv.org/abs/1804.02767"}, "17": {"paper_id": "paper_113", "title": "YOLOv3: An Incremental Improvement", "question_id": 19, "question": "Can we give images of size different than the training to YOLOV3?  ", "question_section": "Class Prediction", "question_trigger_sentence": "Each network is trained with identical settings and tested at , single crop accuracy. Run times are measured on a Titan X at . Thus Darknet-53 performs on par with state-of-the-art classifiers but with fewer floating point operations and more speed. Darknet-53 is better than ResNet-101 and  faster. Darknet-53 has similar performance to ResNet-152 and is  faster.", "question_type": "Testing question", "evidential_info": [], "composition": "The question is not answerable within the materials from the paper, but the conclusion might be that YOLOv3 might work with different image size, because the authors take \"full images\" as input and I guess that the images have at least several different dimensions throughout the dataset. Though, this is never explicitly confirmed in the paper and the question might be unanswerable.", "arxiv_id": "1804.02767", "s2orc_url": "https://www.semanticscholar.org/paper/ebc96892b9bcbf007be9a1d7844e4b09fde9d961", "arxiv_url": "https://arxiv.org/abs/1804.02767"}, "18": {"paper_id": "paper_114", "title": "Unsupervised Learning of Video Representations using LSTMs", "question_id": 1, "question": "Which method out of patch based and convolution net based, performed best for the input of the proposed model for video representation?", "question_section": "Abstract", "question_trigger_sentence": "A general sequence to sequence learning framework was described by Sutskever et al. (2014) in which a recurrent network is used to encode a sequence into a fixed length representation, and then another recurrent network is used to decode a sequence out of that representation. In this work, we apply and extend this framework to learn representations of sequences of images.", "question_type": "shallow question", "evidential_info": [], "composition": "The paper discussed both method combinedly, but never make a comparison between this two. The question is unanswerable from the article.", "arxiv_id": "1502.04681", "s2orc_url": "https://www.semanticscholar.org/paper/829510ad6f975c939d589eeb01a3cf6fc6c8ce4d", "arxiv_url": "https://arxiv.org/abs/1502.04681"}, "19": {"paper_id": "paper_114", "title": "Unsupervised Learning of Video Representations using LSTMs", "question_id": 2, "question": "What is the maximum length of video sequence that the proposed model can handle without any memory issues that are associated with LSTMS?", "question_section": "Our Approach", "question_trigger_sentence": "We use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence.", "question_type": "Testing question ", "evidential_info": [{"context": "We use the UCF-101 and HMDB-51 datasets for supervised tasks.The UCF-101 dataset (Soomro et\u00a0al., 2012) contains 13,320 videos with an average length of6.2 seconds belonging to 101 different action categories. The dataset has 3standard train/test splits with the training set containing around 9,500 videosin each split (the rest are test).The HMDB-51 dataset (Kuehne et\u00a0al., 2011) contains 5100 videos belonging to 51 differentaction categories. Mean length of the videos is 3.2 seconds. This also has 3train/test splits with 3570 videos in the training set and rest in test.", "rationale": "The UCF-101 dataset contains 13,320 videos with an average length of 6.2 seconds.The HMDB-51 dataset contains 5100 videos with mean length of the videos is 3.2 seconds."}], "composition": "For both dataset mean length is mentioned but maximum length is not mentioned in the paper. So it is not answerable.", "arxiv_id": "1502.04681", "s2orc_url": "https://www.semanticscholar.org/paper/829510ad6f975c939d589eeb01a3cf6fc6c8ce4d", "arxiv_url": "https://arxiv.org/abs/1502.04681"}, "20": {"paper_id": "paper_114", "title": "Unsupervised Learning of Video Representations using LSTMs", "question_id": 10, "question": "Is it true that LSTM handles the vanishing gradient problem better than other RNN models?", "question_section": "LSTM Autoencoder Model", "question_trigger_sentence": "Why should this learn good features? The state of the encoder LSTM after the last input has been read is the representation of the input video.", "question_type": "Shallow question ", "evidential_info": [], "composition": "This article doesn't discuss the vanishing gradient problem, hence it is unanswerable from this manuscript.", "arxiv_id": "1502.04681", "s2orc_url": "https://www.semanticscholar.org/paper/829510ad6f975c939d589eeb01a3cf6fc6c8ce4d", "arxiv_url": "https://arxiv.org/abs/1502.04681"}, "21": {"paper_id": "paper_114", "title": "Unsupervised Learning of Video Representations using LSTMs", "question_id": 13, "question": "Is the proposed LSTM Future Predictor Model an autoregressive model?", "question_section": "LSTM Future Predictor Model", "question_trigger_sentence": "This model, on the other hand, predicts a long sequence into the future.", "question_type": "Shallow question ", "evidential_info": [], "composition": "This article doesn't discuss the definition of autoregressive model, hence it is unanswerable from this manuscript.", "arxiv_id": "1502.04681", "s2orc_url": "https://www.semanticscholar.org/paper/829510ad6f975c939d589eeb01a3cf6fc6c8ce4d", "arxiv_url": "https://arxiv.org/abs/1502.04681"}, "22": {"paper_id": "paper_115", "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation", "question_id": 1, "question": "Can the segNet be used for instance segmentation", "question_section": "Introduction", "question_trigger_sentence": "This addresses an important drawback of recent deep learning approaches which have adopted networks designed for object categorization for pixel wise labelling.", "question_type": "Shallow question ", "evidential_info": [], "composition": "The paper does not discussed about instance segmentation. It is hard to infer weather segNet can be used or not for the instance segmentation.", "arxiv_id": "1505.07293", "s2orc_url": "https://www.semanticscholar.org/paper/6f9f143ec602aac743e07d092165b708fa8f1473", "arxiv_url": "https://arxiv.org/abs/1505.07293"}, "23": {"paper_id": "paper_12", "title": "NetVLAD: CNN Architecture for Weakly Supervised Place Recognition", "question_id": 3, "question": "What is \"Fisher vector\"?", "question_section": "Introduction", "question_trigger_sentence": "Each database image is represented using local invariant features [83] such as SIFT [43] that are aggregated into a single vector representation for the entire image such as bag-of-visual-words [74, 53], VLAD [3, 29] or Fisher vector [52, 31]. ", "question_type": "Testing question", "evidential_info": [], "composition": "The paper didn't cover \"Fisher Vector\" definition but generally speaking, it is used to describe a feature map of an image. It encodes the gradients of the log-likelihood of the features under the Gaussian Mixture Model (used to model the distribution of the features extracted from all over the image) w.r.t GMM parameters.", "arxiv_id": "1511.07247", "s2orc_url": "https://www.semanticscholar.org/paper/f971a22287ead6aa23ecd84a4afd8efca57cee3c", "arxiv_url": "https://arxiv.org/abs/1511.07247"}, "24": {"paper_id": "paper_12", "title": "NetVLAD: CNN Architecture for Weakly Supervised Place Recognition", "question_id": 4, "question": "What are the examples of large labelled image datasets?", "question_section": "Introduction", "question_trigger_sentence": "The basic principles of CNNs are known from 80\u2019s [38, 39] and the recent successes are a combination of advances in GPU-based computation power together with large labelled image datasets", "question_type": "Testing question", "evidential_info": [], "composition": "Paper didn't mention the concept in details but one can easily give an example for large labelled image datasets; ImageNet which is divided into train-validation-test data subsets. The training data contains 1000 categories and 1.2 million images.\nAlso, MS COCO, IMDB-Wiki are also examples for large labelled image datasets.", "arxiv_id": "1511.07247", "s2orc_url": "https://www.semanticscholar.org/paper/f971a22287ead6aa23ecd84a4afd8efca57cee3c", "arxiv_url": "https://arxiv.org/abs/1511.07247"}, "25": {"paper_id": "paper_12", "title": "NetVLAD: CNN Architecture for Weakly Supervised Place Recognition", "question_id": 8, "question": "At test time, the visual search is performed by finding the nearest database image to the query, either exactly or through fast approximate nearest neighbor search. What is the benefit of using nearest neighbor search instead of any other search strategy? ", "question_section": "Method overview", "question_trigger_sentence": "At test time, the visual search is performed by finding the nearest database image to the query, either exactly or through fast approximate nearest neighbour search, by sorting images based on the Euclidean distance d(q,i) between f(q) and f(i).", "question_type": "Shallow question", "evidential_info": [], "composition": "Paper didn't discuss answer to this point but it is almost because nearest neighbor search is  fast and efficient considering other strategies in this particular point;It has almost no parameters to make a training burden above all the network parameters. It determines the most similar image in the database to the query image,through simple decision metric at test time. This makes it a great candidate in visual search.It can also be useful with noisy data as it is the case with images obtained from the Google Street View Time Machine.", "arxiv_id": "1511.07247", "s2orc_url": "https://www.semanticscholar.org/paper/f971a22287ead6aa23ecd84a4afd8efca57cee3c", "arxiv_url": "https://arxiv.org/abs/1511.07247"}, "26": {"paper_id": "paper_12", "title": "NetVLAD: CNN Architecture for Weakly Supervised Place Recognition", "question_id": 9, "question": "An alternative setup would be to learn the distance function itself, but the authors chose to fix the distance function to be Euclidean distance. Would results improve with a self-learning distance function? Why or why not?\n", "question_section": "Method overview", "question_trigger_sentence": "An alternative setup would be to learn the distance function itself, but here we choose to fix the distance function to be Euclidean distance, and to pose our problem as the search for the explicit feature map  which works well under the Euclidean distance.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The answer to this point is not discussed in the paper. Results may improve with a self learning distance function, but this would need to be tested and evaluated to judge.", "arxiv_id": "1511.07247", "s2orc_url": "https://www.semanticscholar.org/paper/f971a22287ead6aa23ecd84a4afd8efca57cee3c", "arxiv_url": "https://arxiv.org/abs/1511.07247"}, "27": {"paper_id": "paper_12", "title": "NetVLAD: CNN Architecture for Weakly Supervised Place Recognition", "question_id": 13, "question": "How is the authors' architecture similar to bilinear networks?", "question_section": "Deep architecture for place recognition", "question_trigger_sentence": "Finally, our architecture is also related to bilinear networks [42], recently developed for a different task of fine-grained category-level recognition.", "question_type": "Testing question", "evidential_info": [], "composition": "Question is beyond the scope of the paper which didn't cover a discussion that talked about how the 2 architectures are similar.", "arxiv_id": "1511.07247", "s2orc_url": "https://www.semanticscholar.org/paper/f971a22287ead6aa23ecd84a4afd8efca57cee3c", "arxiv_url": "https://arxiv.org/abs/1511.07247"}, "28": {"paper_id": "paper_12", "title": "NetVLAD: CNN Architecture for Weakly Supervised Place Recognition", "question_id": 15, "question": "What is the size of a panoramic image obtained from Google Street View Time Machine?", "question_section": "Learning from Time Machine data", "question_trigger_sentence": "We propose to exploit a new source of data \u2013 Google Street View Time Machine \u2013 which provides multiple street-level panoramic images taken at different times at close-by spatial locations on the map. As will be seen in section 5.2, this novel data source is precious for learning an image representation for place recognition. ", "question_type": "Testing question", "evidential_info": [], "composition": "Answer is not covered through the paper, however,  images of the dataset are captured by different cameras in different times so the size could easily vary.", "arxiv_id": "1511.07247", "s2orc_url": "https://www.semanticscholar.org/paper/f971a22287ead6aa23ecd84a4afd8efca57cee3c", "arxiv_url": "https://arxiv.org/abs/1511.07247"}, "29": {"paper_id": "paper_12", "title": "NetVLAD: CNN Architecture for Weakly Supervised Place Recognition", "question_id": 17, "question": "How do authors handle the overfitting issue occurring below conv2?", "question_section": "Experiments", "question_trigger_sentence": " The largest improvements are thanks to training the NetVLAD layer, but training other layers results in further improvements, with some overfitting occurring below conv2.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Answer to this question is not discussed in the paper but one can think of dropout and l2 regularization as a general methods for handling overfitting.", "arxiv_id": "1511.07247", "s2orc_url": "https://www.semanticscholar.org/paper/f971a22287ead6aa23ecd84a4afd8efca57cee3c", "arxiv_url": "https://arxiv.org/abs/1511.07247"}, "30": {"paper_id": "paper_120", "title": "Volumetric and Multi-view CNNs for Object Classification on 3D Data", "question_id": 6, "question": "How VoxNet is different than 3DShapeNets ?", "question_section": "Volumetric Convolutional Neural Networks", "question_trigger_sentence": "Additionally, low-frequency information in 3D seems to be quite discriminative for object classification\u2014it is possible to achieve 89:5% accuracy (blue) at a resolution of only 30  30  30.", "question_type": "Testing question", "evidential_info": [], "composition": "It is not possible to answer the question based on the materials from the paper.", "arxiv_id": "1604.03265", "s2orc_url": "https://www.semanticscholar.org/paper/4cdcf2ae5e1fafebd9b3613247a7b1962584da34", "arxiv_url": "https://arxiv.org/abs/1604.03265"}, "31": {"paper_id": "paper_120", "title": "Volumetric and Multi-view CNNs for Object Classification on 3D Data", "question_id": 11, "question": "Why did the authors choose a value of 0.5 for dropout layers in the mlpconv ?", "question_section": "4.3. Network 2: Anisotropic Probing", "question_trigger_sentence": "Without complete knowledge of the object, the auxiliary tasks are more challenging, and can thus better exploit the discriminative power of local regions. ", "question_type": "Deep/complex question", "evidential_info": [], "composition": "It is not possible to infer the answer based on the information from the paper, but my guess is that the authors didn't think much about the specific value of dropout rate; rather 0.5 worked well so they decided to use it.", "arxiv_id": "1604.03265", "s2orc_url": "https://www.semanticscholar.org/paper/4cdcf2ae5e1fafebd9b3613247a7b1962584da34", "arxiv_url": "https://arxiv.org/abs/1604.03265"}, "32": {"paper_id": "paper_122", "title": "Training Very Deep Networks", "question_id": 3, "question": "How can better optimizer help deal with the difficulties of training deep networks ?", "question_section": "Introduction & previous work", "question_trigger_sentence": "Experiments showed that certain activation functions based on local competition [20, 21] may help to train deeper networks;", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The authors introduce related studies that suggest good optimizers that handle deep neural networks well, but are not covered in this paper.", "arxiv_id": "1507.06228", "s2orc_url": "https://www.semanticscholar.org/paper/b92aa7024b87f50737b372e5df31ef091ab54e62", "arxiv_url": "https://arxiv.org/abs/1507.06228"}, "33": {"paper_id": "paper_122", "title": "Training Very Deep Networks", "question_id": 4, "question": "What is an example of action function that may help to train deeper networks ?", "question_section": "Introduction and previous work", "question_trigger_sentence": "Finally, deep networks can be trained layer-wise to help in credit assignment [26, 27], but this approach is less attractive compared to direct training.", "question_type": "Testing question", "evidential_info": [], "composition": "Although related studies on activation functions in deep networks are mentioned, they are not presented in this paper.", "arxiv_id": "1507.06228", "s2orc_url": "https://www.semanticscholar.org/paper/b92aa7024b87f50737b372e5df31ef091ab54e62", "arxiv_url": "https://arxiv.org/abs/1507.06228"}, "34": {"paper_id": "paper_122", "title": "Training Very Deep Networks", "question_id": 5, "question": "Why is training deep networks layer-wise not attractive compared to direct training ?", "question_section": "Introduction and previous work", "question_trigger_sentence": "The stacking of several non-linear transformations in conventional feed-forward network architectures typically results in poor propagation of activations and gradients.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The paper authors say just it is less attractive compared to direct training. There is no reason about this in the paper.", "arxiv_id": "1507.06228", "s2orc_url": "https://www.semanticscholar.org/paper/b92aa7024b87f50737b372e5df31ef091ab54e62", "arxiv_url": "https://arxiv.org/abs/1507.06228"}, "35": {"paper_id": "paper_122", "title": "Training Very Deep Networks", "question_id": 6, "question": "Why does a very deep neural network's stacking of several non linear transformations lead to poor gradient and activation propagation?", "question_section": "Introduction & previous work", "question_trigger_sentence": "This is accomplished through an LSTM-inspired adaptive gating mechanism that allows for computation paths along which information can flow across many layers without attenuation.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The reason why the gradient is bad when a lot of non linear transformations are stacked is not shown in the paper.", "arxiv_id": "1507.06228", "s2orc_url": "https://www.semanticscholar.org/paper/b92aa7024b87f50737b372e5df31ef091ab54e62", "arxiv_url": "https://arxiv.org/abs/1507.06228"}, "36": {"paper_id": "paper_123", "title": "Residual Attention Network for Image Classification", "question_id": 8, "question": "Is there another reason, a part from limited computation resources, that made the authors choose the CIFAR 10/100 datasets for the experiments?", "question_section": "Experiments", "question_trigger_sentence": "The broadly applied state-of-the-art network structure ResNet is used as baseline method.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The paper does not mention any other reason besides limited computation resources for choosing the CIFAR datasets, and therefore the question is unanswerable.", "arxiv_id": "1704.06904", "s2orc_url": "https://www.semanticscholar.org/paper/77d30cf9a34fb6b50979c6a68863099da9a060ad", "arxiv_url": "https://arxiv.org/abs/1704.06904"}, "37": {"paper_id": "paper_123", "title": "Residual Attention Network for Image Classification", "question_id": 10, "question": "Why did the authors add two residual units at each stage ?", "question_section": "Experiments", "question_trigger_sentence": "Since the notion of attention residual learning (ARL) is new, no suitable previous method are comparable therefore we use \u201cnaive attention learning\u201d (NAL) as baseline.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The authors never explain any rationale for why two residual units were added in each stage, so the question is unanswerable.", "arxiv_id": "1704.06904", "s2orc_url": "https://www.semanticscholar.org/paper/77d30cf9a34fb6b50979c6a68863099da9a060ad", "arxiv_url": "https://arxiv.org/abs/1704.06904"}, "38": {"paper_id": "paper_123", "title": "Residual Attention Network for Image Classification", "question_id": 13, "question": "How is the other attention modules (56, 92 and 128) mean absolute response of output features differs than the Attention-164 used?", "question_section": "Experiments", "question_trigger_sentence": "The attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast.", "question_type": "Testing question", "evidential_info": [], "composition": "The authors do not provide any results on the mean absolute response experiment with the other numbers of attention modules. Therefore, we do not know how they were similar/differed.", "arxiv_id": "1704.06904", "s2orc_url": "https://www.semanticscholar.org/paper/77d30cf9a34fb6b50979c6a68863099da9a060ad", "arxiv_url": "https://arxiv.org/abs/1704.06904"}, "39": {"paper_id": "paper_123", "title": "Residual Attention Network for Image Classification", "question_id": 14, "question": "The attention residual learning have a benefit of enhancing the feature constrast, but does it have any inconvenient that is induced by the introduction of additional modules ?", "question_section": "Experiments", "question_trigger_sentence": "We conduct experiments to validate the effectiveness of encoderdecoder structure by comparing with local convolutions without any down sampling or up sampling.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The authors do not mention any drawbacks of their method, and therefore the question is unanswerable.", "arxiv_id": "1704.06904", "s2orc_url": "https://www.semanticscholar.org/paper/77d30cf9a34fb6b50979c6a68863099da9a060ad", "arxiv_url": "https://arxiv.org/abs/1704.06904"}, "40": {"paper_id": "paper_123", "title": "Residual Attention Network for Image Classification", "question_id": 17, "question": "What does \"non-blacklist\" images mean?", "question_section": "Experiments", "question_trigger_sentence": "We evaluate our model using single crop scheme on the ImageNet validation set and show results in Table 7. The Attention-56 network outperforms ResNet-152 by a large margin with a 0:4% reduction on top-1 error and a 0:26% reduction on top-5 error.", "question_type": "Testing question", "evidential_info": [], "composition": "The term \"non-blacklist\" was only mentioned once and was never explained, so this question is unanswerable.", "arxiv_id": "1704.06904", "s2orc_url": "https://www.semanticscholar.org/paper/77d30cf9a34fb6b50979c6a68863099da9a060ad", "arxiv_url": "https://arxiv.org/abs/1704.06904"}, "41": {"paper_id": "paper_123", "title": "Residual Attention Network for Image Classification", "question_id": 18, "question": "Did the authors have an experiment of training the models with different dataset to test the robustness of the attention mechanism ?", "question_section": "Abstract", "question_trigger_sentence": "In this experiment, we show Residual Attention Network can generalize well using different basic unit.", "question_type": "Shallow question", "evidential_info": [], "composition": "The authors do not mention any other datasets, nor do they mention any other experiments. It is therefore impossible to know whether they conducted an experiment with a different dataset, although it seems unlikely.", "arxiv_id": "1704.06904", "s2orc_url": "https://www.semanticscholar.org/paper/77d30cf9a34fb6b50979c6a68863099da9a060ad", "arxiv_url": "https://arxiv.org/abs/1704.06904"}, "42": {"paper_id": "paper_124", "title": "Efficient multi\u2010scale 3D CNN with fully connected CRF for accurate brain lesion segmentation", "question_id": 0, "question": "How does the complexity of brain pathologies vary across a diversified patient population? Are there any demographical, racial or similar features that impacts these variations ?", "question_section": "Introduction", "question_trigger_sentence": "The figure 1 summarizes statistics and shows examples of brain lesions in the case of TBI, but is representative of other pathologies such as brain tumours and ischemic stroke.", "question_type": "Deep/complex question", "evidential_info": [{"context": "The quantitative analysis of lesions requires accurate lesion segmentation in multi-modal, three-dimensional images which is a challenging task for a number of reasons. The heterogeneous appearance of lesions including the large variability in location, size, shape and frequency make it difficult to devise effective segmentation rules.It is thus highly non-trivial to delineate contusions, edema and haemorrhages in TBI (Irimia et\u00a0al. (2012)), or sub-components of brain tumors such as proliferating cells and necrotic core (Menze et\u00a0al. (2015)). The arguably most accurate segmentation results can be obtained through manual delineation by a human expert which is tedious, expensive, time-consuming, impractical in larger studies, and introduces inter-observer variability. Additionally, for deciding whether a particular region is part of a lesion multiple image sequences with varying contrasts need to be considered, and the level of expert knowledge and experience are important factors that impact segmentation accuracy. Hence, in clinical routine often only qualitative, visual inspection, or at best crude measures like approximate lesion volume and number of lesions are used (Yuh et\u00a0al. (2012); Wen et\u00a0al. (2010)). In order to capture and better understand the complexity of brain pathologies it is important to conduct large studies with many subjects to gain the statistical power for drawing conclusions across a whole patient population. The development of accurate, automatic segmentation algorithms has therefore become a major research focus in medical image computing with the potential to offer objective, reproducible, and scalable approaches to quantitative assessment of brain lesions.", "rationale": "Lesions exhibit differences in lesion shape and size, spatial distribution, and frequency. The distribution of normalized intensities over lesions of the same pathology also exhibit a wide variance, with a significant overlap between the lesion distribution and the healthy tissue distribution."}, {"context": "Figure\u00a01 illustrates some of the challenges that arise when devising a computational approach for the task of automatic lesion segmentation. The figure summarizes statistics and shows examples of brain lesions in the case of TBI, but is representative of other pathologies such as brain tumors and ischemic stroke. Lesions can occur at multiple sites, with varying shapes and sizes, and their image intensity profiles largely overlap with non-affected, healthy parts of the brain or lesions which are not in the focus of interest. For example, stroke and MS lesions have a similar hyper-intense appearance in FLAIR sequences as other WMLs (Mitra et\u00a0al. (2014); Schmidt et\u00a0al. (2012)). It is generally difficult to derive statistical prior information about lesion shape and appearance. On the other hand, in some applications there is an expectation on the spatial configuration of segmentation labels, for example there is a hierarchical layout of sub-components in brain tumors. Ideally, a computational approach is able to adjust itself to application specific characteristics by learning from a set of a few example images.", "rationale": "Lesions exhibit differences in lesion shape and size, spatial distribution, and frequency."}], "composition": "Lesions exhibit differences in lesion shape and size, spatial distribution, and frequency. The distribution of normalized intensities over lesions of the same pathology also exhibit a wide variance, with a significant overlap between the lesion distribution and the healthy tissue distribution. The paper does not mention whether any of these variations have any correlation with racial or demographical features, so the second question is unanswerable.", "arxiv_id": "1603.05959", "s2orc_url": "https://www.semanticscholar.org/paper/7c2bcf6f32b05a04cd3444c030db743e5666af88", "arxiv_url": "https://arxiv.org/abs/1603.05959"}, "43": {"paper_id": "paper_124", "title": "Efficient multi\u2010scale 3D CNN with fully connected CRF for accurate brain lesion segmentation", "question_id": 1, "question": "Did the authors have an experiment to claim that the brain lesions in the case of TBI are representative for pathologies such as brain tumours and ischemic stroke ?", "question_section": "Introduction - Contributions", "question_trigger_sentence": "There is a hierarchical layout of sub-components in brain tumours.", "question_type": "Shallow question", "evidential_info": [], "composition": "The paper does not provide any evidence for their claim that the brain lesions for TBI are representative for other pathologies, so the question is unanswerable.", "arxiv_id": "1603.05959", "s2orc_url": "https://www.semanticscholar.org/paper/7c2bcf6f32b05a04cd3444c030db743e5666af88", "arxiv_url": "https://arxiv.org/abs/1603.05959"}, "44": {"paper_id": "paper_124", "title": "Efficient multi\u2010scale 3D CNN with fully connected CRF for accurate brain lesion segmentation", "question_id": 5, "question": "What motivated the choice of this intervals 19^3 to 29^3, why not more or less ?", "question_section": "Analysis of network architecture", "question_trigger_sentence": "This results in an enhanced 9-layers model which we refer to as \u201cDeep + \u201d, and using the same enhancements on the Shallow model yields \u201cShallow + \u201d. The significant performance improvement of Deep + over Shallow + ,", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The authors do not provide a reason for why they chose the intervals between 19^3 and 29^3.", "arxiv_id": "1603.05959", "s2orc_url": "https://www.semanticscholar.org/paper/7c2bcf6f32b05a04cd3444c030db743e5666af88", "arxiv_url": "https://arxiv.org/abs/1603.05959"}, "45": {"paper_id": "paper_124", "title": "Efficient multi\u2010scale 3D CNN with fully connected CRF for accurate brain lesion segmentation", "question_id": 7, "question": "Why did the authors choose only two hidden layers before the classification layer ? did they try making the network deeper by adding more hidden layers ?", "question_section": "Analysis of neural architecture", "question_trigger_sentence": "DeepMedic yields best performance by capturing greater context, while BigDeep + seems to suffer from over-fitting.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The authors do not provide any reason for why they chose two as the number of hidden layers before the final classification. The questions are therefore unanswerable.", "arxiv_id": "1603.05959", "s2orc_url": "https://www.semanticscholar.org/paper/7c2bcf6f32b05a04cd3444c030db743e5666af88", "arxiv_url": "https://arxiv.org/abs/1603.05959"}, "46": {"paper_id": "paper_124", "title": "Efficient multi\u2010scale 3D CNN with fully connected CRF for accurate brain lesion segmentation", "question_id": 13, "question": "The author used only one data augmentation technique ? would the result improve more with using more image augmentation techniques ? Why didn't they try it ?", "question_section": "Evaluation on clinical data", "question_trigger_sentence": "Table 1\r\nPerformance of DeepMedic and an ensemble of three networks on the TBI database. For comparison, we provide results for a Random Forest baseline. Values correspond to the mean (and standard deviation).", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The authors do not mention any other image augmentation techniques and provide no rationale for why they did not try other techniques. The questions are therefore unanswerable.", "arxiv_id": "1603.05959", "s2orc_url": "https://www.semanticscholar.org/paper/7c2bcf6f32b05a04cd3444c030db743e5666af88", "arxiv_url": "https://arxiv.org/abs/1603.05959"}, "47": {"paper_id": "paper_124", "title": "Efficient multi\u2010scale 3D CNN with fully connected CRF for accurate brain lesion segmentation", "question_id": 8, "question": "Deep medic has a better DSC and acurracy than the other models, but why haven't we verified the specificity and sensitivity of the model?", "question_section": "Analysis of network architecture", "question_trigger_sentence": "The performance of the model is not improved, while showing signs of over-fitting.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The authors do not explain why they did not look at specificity and sensitivity of the models when studying the effect of deeper networks, and the question is therefore unanswerable.", "arxiv_id": "1603.05959", "s2orc_url": "https://www.semanticscholar.org/paper/7c2bcf6f32b05a04cd3444c030db743e5666af88", "arxiv_url": "https://arxiv.org/abs/1603.05959"}, "48": {"paper_id": "paper_124", "title": "Efficient multi\u2010scale 3D CNN with fully connected CRF for accurate brain lesion segmentation", "question_id": 14, "question": "From Table 1, we can notice that for the best model Ensemble + CRF, a sensitivity of 63% was achieved. Sensitivity is an important metric to maximize in medical imaging, why didn't the author focused on it specifically ?", "question_section": "Evaluation on clinical data", "question_trigger_sentence": "The network is regularised using dropout ( Hinton et al., 2012 ) with a rate of 2% on all convolutional layers, which is in addition to a 50% rate used on the last two layers.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The authors do not talk about the importance of different metrics and instead focus mostly on improvements in DSC, and therefore the question is unanswerable.", "arxiv_id": "1603.05959", "s2orc_url": "https://www.semanticscholar.org/paper/7c2bcf6f32b05a04cd3444c030db743e5666af88", "arxiv_url": "https://arxiv.org/abs/1603.05959"}, "49": {"paper_id": "paper_124", "title": "Efficient multi\u2010scale 3D CNN with fully connected CRF for accurate brain lesion segmentation", "question_id": 15, "question": "For the TBI experiement, the dropout rate is quite low, were they any overfitting problems related to that ? Did the authors try a higher drop-out rate for a better generalization and performances ?", "question_section": "Evaluation on clinical data", "question_trigger_sentence": "CRF configuration: The parameters of the fully connected CRF [...]", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The authors do not mention any experiment on different dropout rates, and therefore the questions are unanswerable.", "arxiv_id": "1603.05959", "s2orc_url": "https://www.semanticscholar.org/paper/7c2bcf6f32b05a04cd3444c030db743e5666af88", "arxiv_url": "https://arxiv.org/abs/1603.05959"}, "50": {"paper_id": "paper_124", "title": "Efficient multi\u2010scale 3D CNN with fully connected CRF for accurate brain lesion segmentation", "question_id": 16, "question": "The fully connected CRF was used as an image pre-processing technique to improve the performance of the model, did other methods such as HMM, MEMM or even MRF were tested as well ? if else, why not ?", "question_section": "Evaluation on  clinical data", "question_trigger_sentence": "All methods performed worse on the data coming from the second clinical centre, including the method of Feng et al. (2015) that is not machine-learning based", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The paper does not compare any other post-processing technique with the CRF, so the questions are unanswerable.", "arxiv_id": "1603.05959", "s2orc_url": "https://www.semanticscholar.org/paper/7c2bcf6f32b05a04cd3444c030db743e5666af88", "arxiv_url": "https://arxiv.org/abs/1603.05959"}, "51": {"paper_id": "paper_125", "title": "Prompt-to-Prompt Image Editing with Cross Attention Control", "question_id": 2, "question": "Does their method of injecting the cross-attention maps apply only to diffusion-based models, not transformer-based models?", "question_section": "Introduction", "question_trigger_sentence": "Our key idea is that we can edit images by injecting the cross-attention maps during the diffusion process, controlling which pixels attend to which tokens of the prompt text during which diffusion steps. ", "question_type": "Shallow question", "evidential_info": [], "composition": "This question can't be answered using this paper only.", "arxiv_id": "2208.01626", "s2orc_url": "https://www.semanticscholar.org/paper/04e541391e8dce14d099d00fb2c21dbbd8afe87f", "arxiv_url": "https://arxiv.org/abs/2208.01626"}, "52": {"paper_id": "paper_125", "title": "Prompt-to-Prompt Image Editing with Cross Attention Control", "question_id": 12, "question": "How is Imagen different from GLIDE?", "question_section": "Method", "question_trigger_sentence": "Imagen saharia2022photorealistic, similar to GLIDE  nichol2021glide, conditions on the text prompt in the noise prediction of each diffusion step (see section A.2) through two types of attention layers: i) cross-attention layers. ii) hybrid attention that acts both as self-attention and cross-attention by simply concatenating the text embedding sequence to the key-value pairs of each self-attention layer.", "question_type": "Testing question", "evidential_info": [], "composition": "We can't differentiate between Imagen and GLIDE within this paper knowledge, as it requires to deep dive into Imagen and GLIDE papers/architectures.", "arxiv_id": "2208.01626", "s2orc_url": "https://www.semanticscholar.org/paper/04e541391e8dce14d099d00fb2c21dbbd8afe87f", "arxiv_url": "https://arxiv.org/abs/2208.01626"}, "53": {"paper_id": "paper_125", "title": "Prompt-to-Prompt Image Editing with Cross Attention Control", "question_id": 13, "question": "Does each head at the sentence mean each attention head for multi-head attention?", "question_section": "Method", "question_trigger_sentence": "Note that averaging is done for visualization purposes, and attention maps are kept separate for each head in our method.", "question_type": "Shallow question", "evidential_info": [], "composition": "I can't answer this question as It's not clear what does the questioner mean by \"Head at the sentence\", and I need more elaboration to fully answer this question.", "arxiv_id": "2208.01626", "s2orc_url": "https://www.semanticscholar.org/paper/04e541391e8dce14d099d00fb2c21dbbd8afe87f", "arxiv_url": "https://arxiv.org/abs/2208.01626"}, "54": {"paper_id": "paper_125", "title": "Prompt-to-Prompt Image Editing with Cross Attention Control", "question_id": 17, "question": "How did the authors determine the value range of parameter c?", "question_section": "Method", "question_trigger_sentence": " To achieve such manipulation, we scale the attention map of the assigned token j* with parameter c \u2208 [\u22122, 2], resulting in a stronger/weaker effect. ", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Authors didn't mentioned on what base they determined that range of value.", "arxiv_id": "2208.01626", "s2orc_url": "https://www.semanticscholar.org/paper/04e541391e8dce14d099d00fb2c21dbbd8afe87f", "arxiv_url": "https://arxiv.org/abs/2208.01626"}, "55": {"paper_id": "paper_125", "title": "Prompt-to-Prompt Image Editing with Cross Attention Control", "question_id": 18, "question": "Why did the authors use fader control instead of increasing or decreasing percentages?", "question_section": "Application", "question_trigger_sentence": "Instead, we suggest a fader control lample2017fader, where the user controls the magnitude of the effect induced by a specific word, as depicted in fig. 9.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Authors suggested fader control, and didn't mention more about their choice.", "arxiv_id": "2208.01626", "s2orc_url": "https://www.semanticscholar.org/paper/04e541391e8dce14d099d00fb2c21dbbd8afe87f", "arxiv_url": "https://arxiv.org/abs/2208.01626"}, "56": {"paper_id": "paper_127", "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "question_id": 5, "question": "How does the attention mechanism help NTM cope effectively with long input sequences?", "question_section": "Introduction", "question_trigger_sentence": "NMT is often accompanied by an attention mechanism [2] which helps it cope effectively with long input sequences.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "In order to explain how attention mechanism works, we have to dive into other papers that explain attention mechanism which is out of the scope of this paper.", "arxiv_id": "1609.08144", "s2orc_url": "https://www.semanticscholar.org/paper/c6850869aa5e78a107c378d2e8bfa39633158c0c", "arxiv_url": "https://arxiv.org/abs/1609.08144"}, "57": {"paper_id": "paper_127", "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "question_id": 8, "question": "What are the examples of subtle irregularities in the source and target languages?", "question_section": "Model Architecture", "question_trigger_sentence": "In our experiments we found that for NMT systems to achieve good accuracy, both the encoder and decoder RNNs have to be deep enough to capture subtle irregularities in the source and target languages.", "question_type": "Testing question", "evidential_info": [], "composition": "Authors didn't provide examples of subtle irregularities in this paper.", "arxiv_id": "1609.08144", "s2orc_url": "https://www.semanticscholar.org/paper/c6850869aa5e78a107c378d2e8bfa39633158c0c", "arxiv_url": "https://arxiv.org/abs/1609.08144"}, "58": {"paper_id": "paper_128", "title": "Effective Approaches to Attention-based Neural Machine Translation", "question_id": 0, "question": "How can the authors determine the range of the subset of source words they look at once?", "question_section": "Abstract", "question_trigger_sentence": "This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Can't answer this question as there's no mention to picking a range of subset of source words in this paper.", "arxiv_id": "1508.04025", "s2orc_url": "https://www.semanticscholar.org/paper/93499a7c7f699b6630a86fad964536f9423bb6d0", "arxiv_url": "https://arxiv.org/abs/1508.04025"}, "59": {"paper_id": "paper_128", "title": "Effective Approaches to Attention-based Neural Machine Translation", "question_id": 8, "question": "What is the effect of using the source representation s to initialize the decoder hidden state?", "question_section": "Neural Machine Translation", "question_trigger_sentence": "In [Kalchbrenner and Blunsom, 2013, Sutskever et al., 2014, Cho et al., 2014, Luong et al., 2015], the source representation s is only used once to initialize the decoder hidden state. ", "question_type": "Deep/complex question", "evidential_info": [], "composition": "This question can't be answered within the limit of this paper. noting that this question needs more elaboration.", "arxiv_id": "1508.04025", "s2orc_url": "https://www.semanticscholar.org/paper/93499a7c7f699b6630a86fad964536f9423bb6d0", "arxiv_url": "https://arxiv.org/abs/1508.04025"}, "60": {"paper_id": "paper_128", "title": "Effective Approaches to Attention-based Neural Machine Translation", "question_id": 13, "question": "What does \"the horizontally deep network\" mean?", "question_section": "Attention-based Models", "question_trigger_sentence": "The effects of having such connections are two-fold: (a) we hope to make the model fully aware of previous alignment choices and (b) we create a very deep network spanning both horizontally and vertically.", "question_type": "Testing question", "evidential_info": [], "composition": "This question can't be answered within this paper's limit. as it's a basic deep learning term.", "arxiv_id": "1508.04025", "s2orc_url": "https://www.semanticscholar.org/paper/93499a7c7f699b6630a86fad964536f9423bb6d0", "arxiv_url": "https://arxiv.org/abs/1508.04025"}, "61": {"paper_id": "paper_128", "title": "Effective Approaches to Attention-based Neural Machine Translation", "question_id": 14, "question": "How is a horizontally deep network different from a vertically deep network?", "question_section": "Attention-based Models", "question_trigger_sentence": "The effects of having such connections are two-fold: (a) we hope to make the model fully aware of previous alignment choices and (b) we create a very deep network spanning both horizontally and vertically.", "question_type": "Testing question", "evidential_info": [], "composition": "This question can't be answered within this paper's limit. as it's a basic deep learning term.", "arxiv_id": "1508.04025", "s2orc_url": "https://www.semanticscholar.org/paper/93499a7c7f699b6630a86fad964536f9423bb6d0", "arxiv_url": "https://arxiv.org/abs/1508.04025"}, "62": {"paper_id": "paper_128", "title": "Effective Approaches to Attention-based Neural Machine Translation", "question_id": 15, "question": "How can Bahdanau et al. (2015) achieve the \"coverage\" effect using context vectors?", "question_section": "Attention-based Models", "question_trigger_sentence": "Bahdanau et al. (2015) use context vectors, similar to our c_t, in building subsequent hidden states, which can also achieve the \u201ccoverage\u201d effect.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Can't answer this question within the limit of this paper.", "arxiv_id": "1508.04025", "s2orc_url": "https://www.semanticscholar.org/paper/93499a7c7f699b6630a86fad964536f9423bb6d0", "arxiv_url": "https://arxiv.org/abs/1508.04025"}, "63": {"paper_id": "paper_128", "title": "Effective Approaches to Attention-based Neural Machine Translation", "question_id": 16, "question": "What does the \"coverage effect\" mean?", "question_section": "Attention-based Models", "question_trigger_sentence": "Bahdanau et al. (2015) use context vectors, similar to our c_t, in building subsequent hidden\nstates, which can also achieve the \u201ccoverage\u201d effect.", "question_type": "Testing question", "evidential_info": [], "composition": "Can't answer this question within the limit of this paper.", "arxiv_id": "1508.04025", "s2orc_url": "https://www.semanticscholar.org/paper/93499a7c7f699b6630a86fad964536f9423bb6d0", "arxiv_url": "https://arxiv.org/abs/1508.04025"}, "64": {"paper_id": "paper_132", "title": "Video Diffusion Models", "question_id": 10, "question": "What are the implications of such drawback? Does the quality of the video gets worse?", "question_section": "Method", "question_trigger_sentence": "Although samples xb looked good in isolation, they were often not coherent with x a. ", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The Question is ambiguous. I don't understand what \"\"such drawback\"\" indicates.", "arxiv_id": "2204.03458", "s2orc_url": "https://www.semanticscholar.org/paper/3b2a675bb617ae1a920e8e29d535cdf27826e999", "arxiv_url": "https://arxiv.org/abs/2204.03458"}, "65": {"paper_id": "paper_134", "title": "Language Contamination Helps Explain the Cross-lingual Capabilities of English Pretrained Models", "question_id": 6, "question": "Why authors assume English model could not be multilingual?", "question_section": "3. Cross-lingual Transfer of English Pretrained Models", "question_trigger_sentence": "We now ask: how well do models pretrained on these putatively English corpora perform on non-English tasks? ", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Authors do not reason why English models are assumed to be incapable of being multilingual.", "arxiv_id": "2204.08110", "s2orc_url": "https://www.semanticscholar.org/paper/48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775", "arxiv_url": "https://arxiv.org/abs/2204.08110"}, "66": {"paper_id": "paper_134", "title": "Language Contamination Helps Explain the Cross-lingual Capabilities of English Pretrained Models", "question_id": 12, "question": "How many languages used in the experiment for quantification of multilingual ability?", "question_section": "Introduction", "question_trigger_sentence": "More specifically, we quantify how multilingual English pretrained models are in two steps.", "question_type": "Testing question", "evidential_info": [], "composition": "Authors test on 50 languages.", "arxiv_id": "2204.08110", "s2orc_url": "https://www.semanticscholar.org/paper/48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775", "arxiv_url": "https://arxiv.org/abs/2204.08110"}, "67": {"paper_id": "paper_134", "title": "Language Contamination Helps Explain the Cross-lingual Capabilities of English Pretrained Models", "question_id": 21, "question": "To avoid language leakeage problem, the author said what could be the address the issue?", "question_section": "4. Discussion", "question_trigger_sentence": "An obvious follow-up to our findings would be to retrain the models with text that is verified to only contain English data; this would confirm the effect the leaked non-English data has on the models. We reiterate that the standard method for filtering these datasets, automatic language classifiers, is imperfect. ", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The authors do not pose language leakage as a problem.", "arxiv_id": "2204.08110", "s2orc_url": "https://www.semanticscholar.org/paper/48ac4aa7e1d8bfabea4ee39b814fab85aa3e5775", "arxiv_url": "https://arxiv.org/abs/2204.08110"}, "68": {"paper_id": "paper_137", "title": "LAMBADA: Backward Chaining for Automated Reasoning in Natural Language", "question_id": 13, "question": "Why weren\u2019t Proved and unknown combined into one class and attempted?", "question_section": "Implementation details ", "question_trigger_sentence": "similar to SI we combine the UNKNOWN and DISPROVED predic- tions into one class.", "question_type": "Testing question", "evidential_info": [], "composition": "the information is not enough in the paragraphs.", "arxiv_id": "2212.13894", "s2orc_url": "https://www.semanticscholar.org/paper/03fb95e6be583ca954c3d00812a9e9a40f118e51", "arxiv_url": "https://arxiv.org/abs/2212.13894"}, "69": {"paper_id": "paper_137", "title": "LAMBADA: Backward Chaining for Automated Reasoning in Natural Language", "question_id": 14, "question": "What happens if implication of rule and question are different?", "question_section": "Prompts", "question_trigger_sentence": "where <IMLPr> shows the implication of the rule and <IMPLq> indicates the implication of the question. [Xr] and [Xq] are either \u201cpositive\u201c or \u201cnegated\u201c depending on the sign of the im- plication. [Xd] is either \u201cagree\u201c or \u201cdisagree\u201c depending on whether the signs agree or not", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The information is not enough in paragraphs.", "arxiv_id": "2212.13894", "s2orc_url": "https://www.semanticscholar.org/paper/03fb95e6be583ca954c3d00812a9e9a40f118e51", "arxiv_url": "https://arxiv.org/abs/2212.13894"}, "70": {"paper_id": "paper_144", "title": "Hair and Scalp Disease Detection using Machine Learning and Image Processing", "question_id": 5, "question": "What are \"cortisteroids\"?", "question_section": "Challenges and contributions", "question_trigger_sentence": "In severe cases, topical corticosteroid or immune therapy is used", "question_type": "Shallow question", "evidential_info": [], "composition": "None of the given paragraphs is evidential.  Also unable to find any evidential paragraphs in the paper. This question is unanswerable.", "arxiv_id": "2301.00122", "s2orc_url": "https://www.semanticscholar.org/paper/9317eb761d51f6eb428a157a02e2114a90b409e4", "arxiv_url": "https://arxiv.org/abs/2301.00122"}, "71": {"paper_id": "paper_144", "title": "Hair and Scalp Disease Detection using Machine Learning and Image Processing", "question_id": 10, "question": "How many images were collected from each source? Was there any primary data source as well ?", "question_section": "Proposed model", "question_trigger_sentence": " In this study, the authors extracted the images from different websites, such as DermQuest,\nDermNet, MedicineNet, DermnetNZ, and various medical professionals.", "question_type": "Testing question", "evidential_info": [{"context": "The most challenging part of using visual images for disease prediction and disease classification is data collection. Often, one can get fewer appropriate images for a specific illness found. Moreover, the pictures are scattered over the internet. In this study, the authors extracted the images from different websites, such as DermQuest, DermNet, MedicineNet, DermnetNZ, and various medical professionals.", "rationale": "\"the pictures are scattered over the internet. In this study, the authors extracted the images from different websites, such as DermQuest, DermNet, MedicineNet, DermnetNZ, and various medical professionals.\" - There are multiple source of image"}, {"context": "collection, we used iPhone-13 Pro Max having Hexa-core (2x3.23 GHz Avalanche + 4x1.82 GHz Blizzard) CPU and Apple GPU (5-core graphics). We used a mobile device with 128GB 6GB RAM, and a 12 MP triple main camera for the image collection.", "rationale": "\"We used a mobile device with 128GB 6GB RAM, and a 12 MP triple main camera for the image collection.\" - Mobile device is also used for data collection."}], "composition": "There are different image source: websites, such as DermQuest, DermNet, MedicineNet, DermnetNZ, various medical professionals and mobile device. But it is not mentioned how many images were collected from each source. Neither it is mentioned which one is the primary data source. So this question is not answerable correctly.", "arxiv_id": "2301.00122", "s2orc_url": "https://www.semanticscholar.org/paper/9317eb761d51f6eb428a157a02e2114a90b409e4", "arxiv_url": "https://arxiv.org/abs/2301.00122"}, "72": {"paper_id": "paper_145", "title": "A Comparison Study of Deep CNN Architecture in Detecting of Pneumonia", "question_id": 0, "question": "How the reference of plant disease related to detecting pneumonia?", "question_section": "Introduction", "question_trigger_sentence": "To take appropriate action against plant diseases as soon as possible, rapid disease\nidentification models are preferred. ", "question_type": "Testing question", "evidential_info": [], "composition": "None of the paragraph has any information on how plant disease can be used to detect pneumonia. Neither in the paper their is any talk about this reason. This is unanswerable.", "arxiv_id": "2212.14744", "s2orc_url": "https://www.semanticscholar.org/paper/2f22a1b71cd708444c32b0b6310c10497d12f70d", "arxiv_url": "https://arxiv.org/abs/2212.14744"}, "73": {"paper_id": "paper_145", "title": "A Comparison Study of Deep CNN Architecture in Detecting of Pneumonia", "question_id": 8, "question": "What is the necessity of converting colored background to white in all the input images ?", "question_section": "Experiment Description", "question_trigger_sentence": "Images in the dataset were manually checked to ensure they had a white background. Images with colored backgrounds are placed on a white background.", "question_type": "Deep/complex question", "evidential_info": [{"context": "The Pneumonia Image Database provided the data utilized to assess the model's performance. We successfully acquired images from the targeted websites for use in this step. Images in the dataset were manually checked to ensure they had a white background. Images with colored backgrounds are placed on a white background.", "rationale": "Images in the dataset were manually checked to ensure they had a white background. Images with colored backgrounds are placed on a white background."}], "composition": "The author said they used white background in the image, but what is the necessity behind it was never discussed. That is why it is not answerable from this paper.", "arxiv_id": "2212.14744", "s2orc_url": "https://www.semanticscholar.org/paper/2f22a1b71cd708444c32b0b6310c10497d12f70d", "arxiv_url": "https://arxiv.org/abs/2212.14744"}, "74": {"paper_id": "paper_145", "title": "A Comparison Study of Deep CNN Architecture in Detecting of Pneumonia", "question_id": 7, "question": "What is the intution behind taking separate testing and validation data ?", "question_section": "Experiment Description", "question_trigger_sentence": "The dataset is divided into the following 3 folders: train,\ntest, and val. Each image category (Pneumonia/Normal) has\nits own subfolder within the dataset", "question_type": "Shallow question", "evidential_info": [], "composition": "In none of the paragraph the author talked about the intuition behind separating testing and validation data.", "arxiv_id": "2212.14744", "s2orc_url": "https://www.semanticscholar.org/paper/2f22a1b71cd708444c32b0b6310c10497d12f70d", "arxiv_url": "https://arxiv.org/abs/2212.14744"}, "75": {"paper_id": "paper_145", "title": "A Comparison Study of Deep CNN Architecture in Detecting of Pneumonia", "question_id": 11, "question": "How blood cancer cell detection model related to the proposed model ?", "question_section": "Abstract", "question_trigger_sentence": "Following the training model, a model\nfor detecting blood cancer cells was created based on the\nhighest probability of rate, and images of blood cells were\nsorted into distinct disease classes using a softmax output\nlayer", "question_type": "Deep/complex question", "evidential_info": [{"context": "Neural networks (SerensNet152, MobileNetv2, VGG19, ResNet152v2, ResNeXt100 and DenseNet201) was used to detect cell illnesses automatically in this step. Because of its well-known technique as an effective classifier for many real-world applications, the neural network was chosen as a classification tool. Following the training model, a model for detecting blood cancer cells was created based on the highest probability of rate, and images of blood cells were sorted into distinct disease classes using a softmax output layer.", "rationale": "Following the training model, a model for detecting blood cancer cells was created based on the highest probability of rate, and images of blood cells were sorted into distinct disease classes using a softmax output layer."}], "composition": "It is unclear why blood cancer cell detection model is used. Seem it is unrelated to the proposed model. This is not answerable.", "arxiv_id": "2212.14744", "s2orc_url": "https://www.semanticscholar.org/paper/2f22a1b71cd708444c32b0b6310c10497d12f70d", "arxiv_url": "https://arxiv.org/abs/2212.14744"}, "76": {"paper_id": "paper_151", "title": "Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost", "question_id": 11, "question": "SBM-Transformer is inherently stochastic even during inference, and all results presented hold \"in expectation\". Could the authors elaborate more on the numerical stability of the algorithm?", "question_section": "5 Experiments", "question_trigger_sentence": "5 Experiments", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The numerical stability of the algorithm was not discussed in the paper.", "arxiv_id": "2210.15541", "s2orc_url": "https://www.semanticscholar.org/paper/50790468a774f3ecd663e79932e6da4e813048aa", "arxiv_url": "https://arxiv.org/abs/2210.15541"}, "77": {"paper_id": "paper_152", "title": "Grouping-matrix based Graph Pooling with Adaptive Number of Clusters", "question_id": 3, "question": "What do \"fundamental representation\" and \"adjoint representation\" mean?", "question_section": "3 Proposed Method", "question_trigger_sentence": "3 Proposed Method", "question_type": "Testing question", "evidential_info": [], "composition": "The definitions for \"fundamental representation\" and \"adjoint representation\" cannot be found in this paper.", "arxiv_id": "2209.02939", "s2orc_url": "https://www.semanticscholar.org/paper/c7bbe399aad2da39ba8f988fec83129d60aa5d52", "arxiv_url": "https://arxiv.org/abs/2209.02939"}, "78": {"paper_id": "paper_152", "title": "Grouping-matrix based Graph Pooling with Adaptive Number of Clusters", "question_id": 4, "question": "What does \"Abelian\" mean?", "question_section": "3 Proposed Method", "question_trigger_sentence": "3 Proposed Method", "question_type": "Testing question", "evidential_info": [], "composition": "The definition of \"Abelian\" cannot be found in this paper.", "arxiv_id": "2209.02939", "s2orc_url": "https://www.semanticscholar.org/paper/c7bbe399aad2da39ba8f988fec83129d60aa5d52", "arxiv_url": "https://arxiv.org/abs/2209.02939"}, "79": {"paper_id": "paper_152", "title": "Grouping-matrix based Graph Pooling with Adaptive Number of Clusters", "question_id": 10, "question": "Why did the authors manually collect data from scientific articles and patents, when there are many different publicly open datasets for molecular property prediction?", "question_section": "4 Experiments", "question_trigger_sentence": "4 Experiments", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The reason why the authors manually collected data from scientific articles and patents, when there are many publicly open datasets for molecular property prediction, cannot be answered in this paper.", "arxiv_id": "2209.02939", "s2orc_url": "https://www.semanticscholar.org/paper/c7bbe399aad2da39ba8f988fec83129d60aa5d52", "arxiv_url": "https://arxiv.org/abs/2209.02939"}, "80": {"paper_id": "paper_152", "title": "Grouping-matrix based Graph Pooling with Adaptive Number of Clusters", "question_id": 11, "question": "Why did the authors choose to simplify the Tox21 task into a single classification task by setting a positive label if any of the 12 tasks are positive?", "question_section": "4 Experiments", "question_trigger_sentence": "4 Experiments", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The reason why the authors chose to simplify the Tox21 task into a single classification task cannot be answered in this paper.", "arxiv_id": "2209.02939", "s2orc_url": "https://www.semanticscholar.org/paper/c7bbe399aad2da39ba8f988fec83129d60aa5d52", "arxiv_url": "https://arxiv.org/abs/2209.02939"}, "81": {"paper_id": "paper_155", "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models", "question_id": 3, "question": "Sequential Unlearning is more Stable than Batch Unlearning. Do you have any intuition as to why this happens?", "question_section": "Section 4.2", "question_trigger_sentence": "Section 4.2", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Our hypothesis is that when trying to make gradient updates on multiple samples at once, it will result in optimizing towards multiple directions on the loss landscape. Instead, optimizing towards a narrower direction at a time by dividing the multiple samples into different chunks and performing sequential unlearning proves to result in better forgetting without the associated performance degradation.", "arxiv_id": "2210.01504", "s2orc_url": "https://www.semanticscholar.org/paper/91fb2254c5942048425e642c8a6c8d400006150e", "arxiv_url": "https://arxiv.org/abs/2210.01504"}, "82": {"paper_id": "paper_155", "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models", "question_id": 4, "question": "How is the \"hardness\" of forgetting defined. Is it how much predictive performance changes when unlearning an example? Or, is it defined as the number of epochs to successfully achieve forgetting for the given input?", "question_section": "Section 1", "question_trigger_sentence": "Section 1", "question_type": "Testing question", "evidential_info": [], "composition": "In our work, we refer to the \u201chardness\u201d of forgetting as a combination of both the number of epochs needed to achieve successful forgetting and the associated performance degradation that results from the forgetting.", "arxiv_id": "2210.01504", "s2orc_url": "https://www.semanticscholar.org/paper/91fb2254c5942048425e642c8a6c8d400006150e", "arxiv_url": "https://arxiv.org/abs/2210.01504"}, "83": {"paper_id": "paper_155", "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models", "question_id": 5, "question": "Do the authors have any insight into the relation between a model that uses the proposed unlearning mechanism and a model retrained from scratch without the target training examples to be removed?", "question_section": "Section 1", "question_trigger_sentence": "Section 1", "question_type": "Deep/complex question", "evidential_info": [], "composition": "It would be interesting to test the LM retrained from scratch without the target training examples. But as you mentioned, retraining large pretrained LMs from scratch is generally intractable. However, the authors observed that LMs do show a high EL and MA for target token sequences from the validation corpora that were not seen during training, but are very similar to the token sequences that are extractable. This is especially the case for \u2018structured\u2019 token sequences such as code data which have specific patterns that may be repeating. In this scenario, the authors think the difference will not be very significant. On the other hand, further exploring the \u2018generalization\u2019 capability of unlearning is an interesting future work.", "arxiv_id": "2210.01504", "s2orc_url": "https://www.semanticscholar.org/paper/91fb2254c5942048425e642c8a6c8d400006150e", "arxiv_url": "https://arxiv.org/abs/2210.01504"}, "84": {"paper_id": "paper_155", "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models", "question_id": 6, "question": "Why does unlearning a larger batch of examples (128 vs 32) performs significantly worse than sequentially unlearning smaller batches of 32 examples at a time?", "question_section": "Section 4.2", "question_trigger_sentence": "Section 4.2", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Our intuition is that when trying to make gradient updates on multiple samples at once, it will result in optimizing towards multiple directions on the loss landscape. Instead, optimizing towards a narrower direction at a time (sequential unlearning) proves to result in better forgetting without the associated performance degradation.", "arxiv_id": "2210.01504", "s2orc_url": "https://www.semanticscholar.org/paper/91fb2254c5942048425e642c8a6c8d400006150e", "arxiv_url": "https://arxiv.org/abs/2210.01504"}, "85": {"paper_id": "paper_155", "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models", "question_id": 10, "question": "Are there any other work that measures how much knowledge is contained in the parameters of the LMs without checking verbatim generations?", "question_section": "Section 2.3", "question_trigger_sentence": "Section 2.3", "question_type": "Testing question", "evidential_info": [], "composition": "This question is unanswerable since addressing memorization of knowledge without checking verbatim generation it is left for future work.", "arxiv_id": "2210.01504", "s2orc_url": "https://www.semanticscholar.org/paper/91fb2254c5942048425e642c8a6c8d400006150e", "arxiv_url": "https://arxiv.org/abs/2210.01504"}, "86": {"paper_id": "paper_155", "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models", "question_id": 11, "question": "Are there other work that shows gradient ascent boosting performance for the underlying language model ", "question_section": "Abstract", "question_trigger_sentence": "Abstract", "question_type": "Shallow question", "evidential_info": [], "composition": "This question is unanswerable since utilizing gradient ascent for boosting performance it is left for future work.", "arxiv_id": "2210.01504", "s2orc_url": "https://www.semanticscholar.org/paper/91fb2254c5942048425e642c8a6c8d400006150e", "arxiv_url": "https://arxiv.org/abs/2210.01504"}, "87": {"paper_id": "paper_155", "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models", "question_id": 12, "question": "What are some of the benchmarks that measure each individuals privacy standard? ", "question_section": "Section 2.1", "question_trigger_sentence": "Section 2.1", "question_type": "Shallow question", "evidential_info": [], "composition": "This question is unanswerable since there is currently no benchmark that tries to measure individuals' privcay standard.", "arxiv_id": "2210.01504", "s2orc_url": "https://www.semanticscholar.org/paper/91fb2254c5942048425e642c8a6c8d400006150e", "arxiv_url": "https://arxiv.org/abs/2210.01504"}, "88": {"paper_id": "paper_155", "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models", "question_id": 13, "question": "What is the difference between negating the training objective and the unlikelihood training objective proposed in pervious works?", "question_section": "Section 3.1", "question_trigger_sentence": "Section 3.1", "question_type": "Testing question", "evidential_info": [], "composition": "Gradient ascent simply negates the training objective, thus performing maximization of the loss function while unlikelihood training still optimizes for minimization and thus is bounded.", "arxiv_id": "2210.01504", "s2orc_url": "https://www.semanticscholar.org/paper/91fb2254c5942048425e642c8a6c8d400006150e", "arxiv_url": "https://arxiv.org/abs/2210.01504"}, "89": {"paper_id": "paper_155", "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models", "question_id": 14, "question": "Is it possible to perform targeted unlearning to boost performance for individual tasks? ", "question_section": "Section 3.3", "question_trigger_sentence": "Section 3.3", "question_type": "Deep/complex question", "evidential_info": [], "composition": "This question is unaswerable because utilizing unlearning to boost performance for specific tasks is left for future work.", "arxiv_id": "2210.01504", "s2orc_url": "https://www.semanticscholar.org/paper/91fb2254c5942048425e642c8a6c8d400006150e", "arxiv_url": "https://arxiv.org/abs/2210.01504"}, "90": {"paper_id": "paper_155", "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models", "question_id": 16, "question": "Are there any other real-world cases where the problem of LMs memorizing privacy data has led to law suits?", "question_section": "Section 1", "question_trigger_sentence": "Section 1", "question_type": "Shallow question", "evidential_info": [], "composition": "This question is unaswerable because it requires external knowledge of current events.", "arxiv_id": "2210.01504", "s2orc_url": "https://www.semanticscholar.org/paper/91fb2254c5942048425e642c8a6c8d400006150e", "arxiv_url": "https://arxiv.org/abs/2210.01504"}, "91": {"paper_id": "paper_155", "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models", "question_id": 17, "question": "What are some real-world cases where individuals have practiced their RTBF for traditional non neural network systems?", "question_section": "Section 1", "question_trigger_sentence": "Section 1", "question_type": "Shallow question", "evidential_info": [], "composition": "This question is unaswerable because it requires external knowledge of current events.", "arxiv_id": "2210.01504", "s2orc_url": "https://www.semanticscholar.org/paper/91fb2254c5942048425e642c8a6c8d400006150e", "arxiv_url": "https://arxiv.org/abs/2210.01504"}, "92": {"paper_id": "paper_155", "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models", "question_id": 18, "question": "Is there any public data that consists of real world privacy data? ", "question_section": "Section 4.1", "question_trigger_sentence": "Section 4.1", "question_type": "Shallow question", "evidential_info": [], "composition": "This question is unanswerable because it requires external knowledge of the privacy literature.", "arxiv_id": "2210.01504", "s2orc_url": "https://www.semanticscholar.org/paper/91fb2254c5942048425e642c8a6c8d400006150e", "arxiv_url": "https://arxiv.org/abs/2210.01504"}, "93": {"paper_id": "paper_155", "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models", "question_id": 21, "question": "What happens to the threshold value when D\u2019 is varied?", "question_section": "Section 3.2", "question_trigger_sentence": "Section 3.2", "question_type": "Shallow question", "evidential_info": [], "composition": "This question is unanswerable because it requires varying D', but that specific experimental setting has not been explored in the paper.", "arxiv_id": "2210.01504", "s2orc_url": "https://www.semanticscholar.org/paper/91fb2254c5942048425e642c8a6c8d400006150e", "arxiv_url": "https://arxiv.org/abs/2210.01504"}, "94": {"paper_id": "paper_155", "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models", "question_id": 23, "question": "Why does pretraining LMs with Differential Privacy result in more computational overhead? ", "question_section": "Section 2.1", "question_trigger_sentence": "Section 2.1", "question_type": "Shallow question", "evidential_info": [], "composition": "Pretraining LMs with DP results in more computational overhead because it requires calculating multiple gradients to inject noise during optimization.", "arxiv_id": "2210.01504", "s2orc_url": "https://www.semanticscholar.org/paper/91fb2254c5942048425e642c8a6c8d400006150e", "arxiv_url": "https://arxiv.org/abs/2210.01504"}, "95": {"paper_id": "paper_156", "title": "Translating Melody to Chord: Structured and Flexible Harmonization of Melody with Transformer", "question_id": 4, "question": "Is \"discriminative learning\" the correct term to represent the following previous works?", "question_section": "2. Related Works > C. Music Generation from Discriminative Learning", "question_trigger_sentence": "Discriminative learning frameworks have been adapted to music generation studies. ", "question_type": "Shallow question", "evidential_info": [], "composition": "Why the authors used average pooling instead of other methods to make the notewise embedding in the TimeToNote procedure cannot be answered in this paper.", "s2orc_id": "247185306", "s2orc_url": "https://www.semanticscholar.org/paper/75b0c527a6e478523477d175b4b148ba88db0995", "arxiv_url": null}, "96": {"paper_id": "paper_156", "title": "Translating Melody to Chord: Structured and Flexible Harmonization of Melody with Transformer", "question_id": 12, "question": "The genres of the datasets seem to be constrained to some contemporary ones. How can using these datasets verify the general performance of the proposed models?", "question_section": "4. Experimental Settings > A. Datasets", "question_trigger_sentence": "HLSD [13] is an online database of melody and chord annotations that cover various genres, such as the pop, new age, and original soundtracks.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "How using datasets with constrained genres can verify the general performance of the proposed models cannot be answered in this paper.", "s2orc_id": "247185306", "s2orc_url": "https://www.semanticscholar.org/paper/75b0c527a6e478523477d175b4b148ba88db0995", "arxiv_url": null}, "97": {"paper_id": "paper_156", "title": "Translating Melody to Chord: Structured and Flexible Harmonization of Melody with Transformer", "question_id": 15, "question": "Would it be fair to use the baseline models which have a much smaller number of parameters and a much simpler model architecture to evaluate the proposed models?", "question_section": "4. Experimental Settings > B. Comparative Methods", "question_trigger_sentence": "BLSTM by Lim et al. [9] is composed of two stacked layers of bidirectional LSTM.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Whether it would be fair to use the baseline models which have a much smaller number of parameters and a much simpler model architecture to evaluate the proposed models cannot be answered in this paper.", "s2orc_id": "247185306", "s2orc_url": "https://www.semanticscholar.org/paper/75b0c527a6e478523477d175b4b148ba88db0995", "arxiv_url": null}, "98": {"paper_id": "paper_156", "title": "Translating Melody to Chord: Structured and Flexible Harmonization of Melody with Transformer", "question_id": 18, "question": "Why does STHarm show the lowest scores in chord diversity compared to the baseline models?", "question_section": "5. Evaluation > A. Chord Coherence and Diveristy", "question_trigger_sentence": "STHarm, on the other hand, reveals the lowest CTD and the lowest CHE and CC for all datasets except for CHE on HLSD. ", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Why STHarm shows the lowest scores in chord diversity compared to the baseline models cannot be answered in this paper.", "s2orc_id": "247185306", "s2orc_url": "https://www.semanticscholar.org/paper/75b0c527a6e478523477d175b4b148ba88db0995", "arxiv_url": null}, "99": {"paper_id": "paper_156", "title": "Translating Melody to Chord: Structured and Flexible Harmonization of Melody with Transformer", "question_id": 21, "question": "What is a numerical setting of tSNE when visualizing z?", "question_section": "5. Evaluation > C. Controlling Chord Complexity", "question_trigger_sentence": "Then, the dimension of z is reduced by two with t-stochastic neighbor embedding (tSNE) [30]. ", "question_type": "Testing question", "evidential_info": [], "composition": "The numerical setting of tSNE for visualizing z cannot be answered in this paper.", "s2orc_id": "247185306", "s2orc_url": "https://www.semanticscholar.org/paper/75b0c527a6e478523477d175b4b148ba88db0995", "arxiv_url": null}, "100": {"paper_id": "paper_156", "title": "Translating Melody to Chord: Structured and Flexible Harmonization of Melody with Transformer", "question_id": 24, "question": "What does \"musical backgrounds\" mean?", "question_section": "5. Evaluation > D. Subjective Evaluation", "question_trigger_sentence": "Thirtytwo participants indicated that they had musical backgrounds, and 25 participants mentioned that they usually listened to popular music.", "question_type": "Testing question", "evidential_info": [], "composition": "The concrete meaning of \"musical backgrounds\" cannot be answered in this paper.", "s2orc_id": "247185306", "s2orc_url": "https://www.semanticscholar.org/paper/75b0c527a6e478523477d175b4b148ba88db0995", "arxiv_url": null}, "101": {"paper_id": "paper_156", "title": "Translating Melody to Chord: Structured and Flexible Harmonization of Melody with Transformer", "question_id": 6, "question": "Why did the authors use average pooling, instead of other methods such as max pooling, to make the notewise embedding in the TimeToNote procedure?", "question_section": "3. Proposed Method > A. STHarm", "question_trigger_sentence": "Then, we transfer it to the notewise embedding with average pooling by an alignment matrix", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Why the authors used average pooling instead of other methods to make the node-wise embedding in the TimeToNote procedure cannot be answered in this paper.", "s2orc_id": "247185306", "s2orc_url": "https://www.semanticscholar.org/paper/75b0c527a6e478523477d175b4b148ba88db0995", "arxiv_url": null}, "102": {"paper_id": "paper_157", "title": "Sketching the Expression: Flexible Rendering of Expressive Piano Performance with Self-Supervised Learning", "question_id": 5, "question": "There are too many hyperparameters in the training objective. Would it be any possibility that the model is more affected by hyperparameters than the regularization terms themselves?", "question_section": "2. Proposed Methods > 2.5. Overall Objective", "question_trigger_sentence": "2. Proposed Methods > 2.5. Overall Objective", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Whether the model can be more affected by hyperparameters than the regularization terms themselves cannot be answered in this paper.", "arxiv_id": "2208.14867", "s2orc_url": "https://www.semanticscholar.org/paper/51413f1700db809e0481acbe89d9fd3d5f5cfb25", "arxiv_url": "https://arxiv.org/abs/2208.14867"}, "103": {"paper_id": "paper_157", "title": "Sketching the Expression: Flexible Rendering of Expressive Piano Performance with Self-Supervised Learning", "question_id": 7, "question": "What are the 10 composers that are chosen from the ASAP dataset?", "question_section": "3. Experimental Setups > 3.1. Dataset and Implementation", "question_trigger_sentence": "3. Experimental Setups > 3.1. Dataset and Implementation", "question_type": "Shallow question", "evidential_info": [], "composition": "The 10 composers chosen from the ASAP dataset cannot be answered in this paper.", "arxiv_id": "2208.14867", "s2orc_url": "https://www.semanticscholar.org/paper/51413f1700db809e0481acbe89d9fd3d5f5cfb25", "arxiv_url": "https://arxiv.org/abs/2208.14867"}, "104": {"paper_id": "paper_157", "title": "Sketching the Expression: Flexible Rendering of Expressive Piano Performance with Self-Supervised Learning", "question_id": 11, "question": "What would be a reason that the model without L(reg) outperforms the restrictiveness of the model with L(reg)?", "question_section": "4. Evaluation > 4.3. Controllability of Expressive Attributes", "question_trigger_sentence": "4. Evaluation > 4.3. Controllability of Expressive Attributes", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The reason that the model without L(reg) outperforms the restrictiveness of the model with L(reg) cannot be answered in this paper.", "arxiv_id": "2208.14867", "s2orc_url": "https://www.semanticscholar.org/paper/51413f1700db809e0481acbe89d9fd3d5f5cfb25", "arxiv_url": "https://arxiv.org/abs/2208.14867"}, "105": {"paper_id": "paper_162", "title": "Self-supervised auxiliary learning with meta-paths for heterogeneous graphs", "question_id": 2, "question": "Why did most GNNs only work on homogeneous graphs and not heterogeneous graphs?", "question_section": "Introduction", "question_trigger_sentence": "Since most graph neural networks operate on homogeneous graphs, which have a single type of nodes and edges, the previous pre-training/auxiliary tasks are not specifically designed for heterogeneous graphs, which have multiple types of nodes and edges", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The answer to the question could not be found in the paper.", "arxiv_id": "2007.08294", "s2orc_url": "https://www.semanticscholar.org/paper/4ccdd1fe6cc9c896e910582bea2c33c19317c5ca", "arxiv_url": "https://arxiv.org/abs/2007.08294"}, "106": {"paper_id": "paper_162", "title": "Self-supervised auxiliary learning with meta-paths for heterogeneous graphs", "question_id": 9, "question": "How are the meta-paths utilized and studied outside the context of self-supervised learning?", "question_section": "Method", "question_trigger_sentence": "To our knowledge, the meta-path prediction has not been studied in the context of self-supervised learning for graph neural networks in the literature.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The answer to the question could not be found in the paper.", "arxiv_id": "2007.08294", "s2orc_url": "https://www.semanticscholar.org/paper/4ccdd1fe6cc9c896e910582bea2c33c19317c5ca", "arxiv_url": "https://arxiv.org/abs/2007.08294"}, "107": {"paper_id": "paper_162", "title": "Self-supervised auxiliary learning with meta-paths for heterogeneous graphs", "question_id": 11, "question": "What is the effect of concatenating the one-hot representation of task types and sample labels as inputs to the weight function?", "question_section": "Method", "question_trigger_sentence": "It is the concatenation of one-hot representation of task types, the label of the sample (positive/negative), and its loss value, i.e., \u03be (t,train) i = h ` t ; et; y (t,train) i i \u2208 R T +2.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The paper does not contain a response to the question.", "arxiv_id": "2007.08294", "s2orc_url": "https://www.semanticscholar.org/paper/4ccdd1fe6cc9c896e910582bea2c33c19317c5ca", "arxiv_url": "https://arxiv.org/abs/2007.08294"}, "108": {"paper_id": "paper_162", "title": "Self-supervised auxiliary learning with meta-paths for heterogeneous graphs", "question_id": 16, "question": "They claim that their approach is the first to generate meta-paths from heterogeneous graphs without additional labeling and use them as labels. Is this true?", "question_section": "Introduction", "question_trigger_sentence": "To the best of our knowledge, this is the first auxiliary task with meta-paths specifically designed for leveraging heterogeneous graph structure.", "question_type": "Shallow question", "evidential_info": [], "composition": "Although the paper mentions it, answering the question with only the given information in the paper is difficult.", "arxiv_id": "2007.08294", "s2orc_url": "https://www.semanticscholar.org/paper/4ccdd1fe6cc9c896e910582bea2c33c19317c5ca", "arxiv_url": "https://arxiv.org/abs/2007.08294"}, "109": {"paper_id": "paper_163", "title": "Point cloud augmentation with weighted local transformations", "question_id": 5, "question": "Is there a way to quantitatively measure the visually explainable augmentation in local transformations?", "question_section": "Experiment", "question_trigger_sentence": "Importantly, seeing how these visually explainable augmentations from local transformations also bring empirical benefits, understanding and exploiting local structures are crucial for successful DA on point cloud.", "question_type": "Shallow question", "evidential_info": [], "composition": "The paper does not mention anything about that part.", "arxiv_id": "2110.05379", "s2orc_url": "https://www.semanticscholar.org/paper/4e6bbeee3e8810de2a5b12941f81920866a6b38b", "arxiv_url": "https://arxiv.org/abs/2110.05379"}, "110": {"paper_id": "paper_175", "title": "Unsupervised Visual Representation Learning via Mutual Information Regularized Assignment", "question_id": 7, "question": "Is the improvement of self-supervised learning methods negligible compared to the other criteria (e.g. network)?", "question_section": "Main results", "question_trigger_sentence": "Tables 1 and 2 report linear evaluation results.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The goal is not on the paper. More on the motivation of the field.", "arxiv_id": "2211.02284", "s2orc_url": "https://www.semanticscholar.org/paper/69ac3c64efb9a872ce8eaca7f4c64c06ecb3c3f1", "arxiv_url": "https://arxiv.org/abs/2211.02284"}, "111": {"paper_id": "paper_175", "title": "Unsupervised Visual Representation Learning via Mutual Information Regularized Assignment", "question_id": 9, "question": "Does the convergence into the optimal pseudo-label directly related to the performance? SwaV does adequately well on self-supervised experiments.", "question_section": "Analysis", "question_trigger_sentence": "Figure 2 shows the result of the converging behavior of our method (blue) and SK algorithm (yellow)\non trained models of MIRA (left) and SwAV (right).", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Not in the paper.", "arxiv_id": "2211.02284", "s2orc_url": "https://www.semanticscholar.org/paper/69ac3c64efb9a872ce8eaca7f4c64c06ecb3c3f1", "arxiv_url": "https://arxiv.org/abs/2211.02284"}, "112": {"paper_id": "paper_175", "title": "Unsupervised Visual Representation Learning via Mutual Information Regularized Assignment", "question_id": 11, "question": "What is the motivation for K-NN evaluation?", "question_section": "Main results", "question_trigger_sentence": "We evaluate the quality of learned representation via the nearest neighbor classifier.\n", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Not on the scope of the paper.", "arxiv_id": "2211.02284", "s2orc_url": "https://www.semanticscholar.org/paper/69ac3c64efb9a872ce8eaca7f4c64c06ecb3c3f1", "arxiv_url": "https://arxiv.org/abs/2211.02284"}, "113": {"paper_id": "paper_175", "title": "Unsupervised Visual Representation Learning via Mutual Information Regularized Assignment", "question_id": 19, "question": "Why ablation on EMA and MC parameters is not available instead of following the previous practices?", "question_section": "Analysis", "question_trigger_sentence": "Table 6 reports an ablation study on how EMA and multi-crop augmentations\naffect our representation quality", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Not answerable. Context of the question is more general than the scope of the paper.", "arxiv_id": "2211.02284", "s2orc_url": "https://www.semanticscholar.org/paper/69ac3c64efb9a872ce8eaca7f4c64c06ecb3c3f1", "arxiv_url": "https://arxiv.org/abs/2211.02284"}, "114": {"paper_id": "paper_175", "title": "Unsupervised Visual Representation Learning via Mutual Information Regularized Assignment", "question_id": 23, "question": "How much batch size is used for other self-supervised learning methods? ", "question_section": "Implementation details", "question_trigger_sentence": "We use a batch size of 4096 and employ the LARS optimizer [52] with a\nweight decay of 10\u22126\n.", "question_type": "Testing question", "evidential_info": [], "composition": "Not answerable.", "arxiv_id": "2211.02284", "s2orc_url": "https://www.semanticscholar.org/paper/69ac3c64efb9a872ce8eaca7f4c64c06ecb3c3f1", "arxiv_url": "https://arxiv.org/abs/2211.02284"}, "115": {"paper_id": "paper_175", "title": "Unsupervised Visual Representation Learning via Mutual Information Regularized Assignment", "question_id": 22, "question": "What is the true label set of the ImagNet dataset? How did you set the pseudo-label set of MIRA?", "question_section": "Implementation details", "question_trigger_sentence": "s We train our model on the training set of the ILSVRC-2012 ImageNet-1k\ndataset [18] without using class labels.", "question_type": "Testing question", "evidential_info": [], "composition": "We use d = 256 and K = 3000.", "arxiv_id": "2211.02284", "s2orc_url": "https://www.semanticscholar.org/paper/69ac3c64efb9a872ce8eaca7f4c64c06ecb3c3f1", "arxiv_url": "https://arxiv.org/abs/2211.02284"}, "116": {"paper_id": "paper_182", "title": "S3NAS: Fast NPU-aware Neural Architecture Search Methodology", "question_id": 1, "question": "How much accuracy the linear supernet design (author's proposal) gives on MIDAP?", "question_section": "Supernet Design", "question_trigger_sentence": "Thus, we place more blocks to stages with larger width in the\nsupernet, making the cumulative depth up to a specific stage is\nproportional to the width of the stage, which is similar to PyramidNet [10]", "question_type": "Deep/complex question", "evidential_info": [], "composition": "1.24%. The result can be found in Table 4.", "arxiv_id": "2009.02009", "s2orc_url": "https://www.semanticscholar.org/paper/58ae40775b704ca0d9ad7c0fd8f0bb7e088f1295", "arxiv_url": "https://arxiv.org/abs/2009.02009"}, "117": {"paper_id": "paper_182", "title": "S3NAS: Fast NPU-aware Neural Architecture Search Methodology", "question_id": 11, "question": "How much latency reduction authors obtained through selective removal of 'lightweight' SE blocks?", "question_section": "Selective removal of SE", "question_trigger_sentence": "Figure 11 depicts an example distribution of activation values produced by two different SE blocks\nfor three different images. The authors of the original paper [13]\nconjectured that if such distribution from a SE block does not differ\nwidely between image classes, the SE block is not important. Thus,\nafter training, they obtained averaged activation values of a SE\nblock over multiple images in the same class. They compared the\ndistributions of the averaged values over different image classes.\nThey observed that removing the SE blocks that have similar distributions over different image classes incurs only a marginal loss in\naccuracy.\n", "question_type": "Testing question", "evidential_info": [], "composition": "0.21ms. The answer can be found in Table 6.", "arxiv_id": "2009.02009", "s2orc_url": "https://www.semanticscholar.org/paper/58ae40775b704ca0d9ad7c0fd8f0bb7e088f1295", "arxiv_url": "https://arxiv.org/abs/2009.02009"}, "118": {"paper_id": "paper_182", "title": "S3NAS: Fast NPU-aware Neural Architecture Search Methodology", "question_id": 12, "question": "Why authors are focusing on quantization this much?", "question_section": "Add h-swish and SE", "question_trigger_sentence": "In addition to compound scaling, we\nadd two components in the post-processing step: h-swish activation\nfunction and squeeze-and-excitation (SE) block. A recent study [23]\nreports that SE and the h-swish activation function are no hurdles\nfor 8-bit quantization. They could quantize a network with SE and\nh-swish without noticeable accuracy loss.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "It is not answerable based on the paper content.", "arxiv_id": "2009.02009", "s2orc_url": "https://www.semanticscholar.org/paper/58ae40775b704ca0d9ad7c0fd8f0bb7e088f1295", "arxiv_url": "https://arxiv.org/abs/2009.02009"}, "119": {"paper_id": "paper_185", "title": "Adversarial TableQA: Attention Supervision for Question Answering on Tables", "question_id": 0, "question": "What do \"spurious\" programs mean?", "question_section": "Introduction", "question_trigger_sentence": "This leads to confusion in the model on which of the possible ways should it choose to arrive at the final answer. For example, wrong cells can be in the same column (i.e. (d) in the figure), or they can be in a similar column using the same operation (i.e. (e) in the figure).", "question_type": "Testing question", "evidential_info": [], "composition": "\"Spurious\u201d programs accidentally lead to the correct answers even by using the wrong cells or operations, as correct answers.", "arxiv_id": "1810.08113", "s2orc_url": "https://www.semanticscholar.org/paper/5f2bb157c66870e5abf3d0873888c231445bd16a", "arxiv_url": "https://arxiv.org/abs/1810.08113"}, "120": {"paper_id": "paper_185", "title": "Adversarial TableQA: Attention Supervision for Question Answering on Tables", "question_id": 1, "question": "How is similar WikiOps to WikiSQL?", "question_section": "Our dataset", "question_trigger_sentence": "To compare and contrast SQL with its operand annotations, we present WikiOps an altered version of the original WikiSQL dataset (Zhong et al., 2017)", "question_type": "Testing question", "evidential_info": [], "composition": "WikiOps is an altered version of the original WikiSQL dataset, by transforming the SQL statements into operand information.", "arxiv_id": "1810.08113", "s2orc_url": "https://www.semanticscholar.org/paper/5f2bb157c66870e5abf3d0873888c231445bd16a", "arxiv_url": "https://arxiv.org/abs/1810.08113"}, "121": {"paper_id": "paper_185", "title": "Adversarial TableQA: Attention Supervision for Question Answering on Tables", "question_id": 2, "question": "Did the authors introduce new datasets?", "question_section": "Our dataset", "question_trigger_sentence": "We perform preliminary analysis based on the current datasets, with the following insight on the gap of real-life and synthetic queries.", "question_type": "Shallow question", "evidential_info": [], "composition": "Yes, they create and introduce two new datasets called MLB dataset and WikiOps dataset.", "arxiv_id": "1810.08113", "s2orc_url": "https://www.semanticscholar.org/paper/5f2bb157c66870e5abf3d0873888c231445bd16a", "arxiv_url": "https://arxiv.org/abs/1810.08113"}, "122": {"paper_id": "paper_185", "title": "Adversarial TableQA: Attention Supervision for Question Answering on Tables", "question_id": 3, "question": "Does the proposed model consider the aggregation of multiple cells or just for one cell?", "question_section": "Our model: Neural Operator", "question_trigger_sentence": "In this section, we present our model Neural Operator (NeOp). NeOp is a multi-layer sequential network that accepts as input the query and the given table, and learns both the operands and the operation to calculate the final answer.", "question_type": "Shallow question", "evidential_info": [], "composition": "The proposed model allows operation on multiple cells and is extensible to new operations.", "arxiv_id": "1810.08113", "s2orc_url": "https://www.semanticscholar.org/paper/5f2bb157c66870e5abf3d0873888c231445bd16a", "arxiv_url": "https://arxiv.org/abs/1810.08113"}, "123": {"paper_id": "paper_185", "title": "Adversarial TableQA: Attention Supervision for Question Answering on Tables", "question_id": 4, "question": "What are the metrics did the authors use in their experiments on MLB dataset?", "question_section": "MLB dataset", "question_trigger_sentence": "We compare our models with three competing TableQA models: one semantic parsing-based model, Sempre (Pasupat and Liang, 2015), and two neural-based models, Neural Enquirer NeEn (Yin et al., 2016) and Neural Programmer NePr (Neelakantan et al., 2016). We use the MLB dataset and it consists of 16k train set, 10k development set and 10k test set", "question_type": "Shallow question", "evidential_info": [], "composition": "They evaluate the models using four metrics on MLB dataset: SoftOpP, SoftOpR, HardOpA, FinalAcc.", "arxiv_id": "1810.08113", "s2orc_url": "https://www.semanticscholar.org/paper/5f2bb157c66870e5abf3d0873888c231445bd16a", "arxiv_url": "https://arxiv.org/abs/1810.08113"}, "124": {"paper_id": "paper_185", "title": "Adversarial TableQA: Attention Supervision for Question Answering on Tables", "question_id": 5, "question": "Why are sql annotations hard to get?", "question_section": "Introduction", "question_trigger_sentence": "Recently, SQL statements were proposed as annotations to improve the performance of models (Zhong et al., 2017). They can uniquely define the answer in Figure 1, and hence not spurious, but with the following overheads", "question_type": "Deep/complex question", "evidential_info": [], "composition": "As more complex the query, SQL annotations can only be obtained from SQL experts, thus crowdsourcing of training resources from normal non technical users is limited.", "arxiv_id": "1810.08113", "s2orc_url": "https://www.semanticscholar.org/paper/5f2bb157c66870e5abf3d0873888c231445bd16a", "arxiv_url": "https://arxiv.org/abs/1810.08113"}, "125": {"paper_id": "paper_185", "title": "Adversarial TableQA: Attention Supervision for Question Answering on Tables", "question_id": 7, "question": "Why did the authors introduce new annotations for the TableQA dataset?", "question_section": "Introduction", "question_trigger_sentence": "In this paper, we propose attention supervision that does not require SQL expertise. Instead, for attention supervision, we propose to use \u201coperand information\u201d, the set of correct cells to be selected and operated that provides much richer information about the final answer.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "In tableQA, using only answer annotation is too weak for both evaluation and training purposes. So, additional annotations are needed, but SQL statements are difficult to obtain from non-technical users and their expression is very limited.", "arxiv_id": "1810.08113", "s2orc_url": "https://www.semanticscholar.org/paper/5f2bb157c66870e5abf3d0873888c231445bd16a", "arxiv_url": "https://arxiv.org/abs/1810.08113"}, "126": {"paper_id": "paper_185", "title": "Adversarial TableQA: Attention Supervision for Question Answering on Tables", "question_id": 8, "question": "How did authors represent the cell value to include header information in each cell value of the table?", "question_section": "Encoding", "question_trigger_sentence": "We use a single word embedding matrix  for the initial vectors of both space-separated strings in the query and cells in the table.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The authors combine the word embedding of each table cell with the column header vector to create the final cell vector.", "arxiv_id": "1810.08113", "s2orc_url": "https://www.semanticscholar.org/paper/5f2bb157c66870e5abf3d0873888c231445bd16a", "arxiv_url": "https://arxiv.org/abs/1810.08113"}, "127": {"paper_id": "paper_185", "title": "Adversarial TableQA: Attention Supervision for Question Answering on Tables", "question_id": 9, "question": "How do they show that interpretability of the proposed model?", "question_section": "Model interpretability", "question_trigger_sentence": "Through the Column, Pivot, Param SelRUs, and the Operation Solver of NeOp, it is easy to interpret the model in a step-by-step manner. ", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The authors showed the interpretability of the model in the following process. First, at each timestep, they obtain the attention weights of the Attentive Pooling component of each SelRU, and the one with the highest weight among them is the selected field/word. Second, after the final timestep, they obtain the attention weights of the operation solver, and the one with the highest weight among them is the selected operation.", "arxiv_id": "1810.08113", "s2orc_url": "https://www.semanticscholar.org/paper/5f2bb157c66870e5abf3d0873888c231445bd16a", "arxiv_url": "https://arxiv.org/abs/1810.08113"}, "128": {"paper_id": "paper_185", "title": "Adversarial TableQA: Attention Supervision for Question Answering on Tables", "question_id": 10, "question": "Why is the proposed model robust against adversarial examples?", "question_section": "Robustness to adversarial examples", "question_trigger_sentence": "For V-P, NeOp stays robust, only with a slight degradation (0.25%) from its original accuracy. However, Sempre and NeOp show more significant decreases of 2.80% and 3.70% from their original accuracy, respectively. For O-P, NePr is still the most sensitive to adversarial examples with the decrease of 1.23%.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Other neural models cannot distinguish the right answer derived from spurious programs, resulting in confusion in learning. However, the proposed model does not, and can be trained correctly. So, the proposed model is robust to adversarial examples.", "arxiv_id": "1810.08113", "s2orc_url": "https://www.semanticscholar.org/paper/5f2bb157c66870e5abf3d0873888c231445bd16a", "arxiv_url": "https://arxiv.org/abs/1810.08113"}, "129": {"paper_id": "paper_185", "title": "Adversarial TableQA: Attention Supervision for Question Answering on Tables", "question_id": 6, "question": "How does operand loss affect the model's performance?", "question_section": "Analysis on operand loss", "question_trigger_sentence": "We argue that one of the reasons why NeOp performs well is that we use the operand information for solving the queries.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Adding the operand loss doubles the model's hard operand accuracy and increases the final accuracy by about 50%.", "arxiv_id": "1810.08113", "s2orc_url": "https://www.semanticscholar.org/paper/5f2bb157c66870e5abf3d0873888c231445bd16a", "arxiv_url": "https://arxiv.org/abs/1810.08113"}, "130": {"paper_id": "paper_2", "title": "Exploring the Limits of Language Modeling", "question_id": 1, "question": "What is the significance of a BLEU score? ", "question_section": "Introduction", "question_trigger_sentence": "Often (although not always), training better language models improves the underlying metrics of the downstream task (such as word error rate for speech recognition, or BLEU score for translation), which makes the task of training better LMs valuable by itself.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The BLEU score is a performance metric for machine translation tasks.", "arxiv_id": "1602.02410", "s2orc_url": "https://www.semanticscholar.org/paper/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "arxiv_url": "https://arxiv.org/abs/1602.02410"}, "131": {"paper_id": "paper_2", "title": "Exploring the Limits of Language Modeling", "question_id": 6, "question": "What does the context vector represent and what values does it contain? ", "question_section": "2.3 Softmax Over Large Vocabularies", "question_trigger_sentence": "The logit is generally computed as an inner product  where h is a context vector and e_w is a \u201cword embedding\u201d for w.", "question_type": "Shallow question", "evidential_info": [], "composition": "The paper does not define the context vector nor give/interpret its values.", "arxiv_id": "1602.02410", "s2orc_url": "https://www.semanticscholar.org/paper/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "arxiv_url": "https://arxiv.org/abs/1602.02410"}, "132": {"paper_id": "paper_2", "title": "Exploring the Limits of Language Modeling", "question_id": 7, "question": "What do the two probabilities in the decomposition represent?", "question_section": "3.1 Relationship between Noise Contrastive Estimation and Importance Sampling", "question_trigger_sentence": "A Hierarchical Softmax (Mnih & Hinton, 2009) employs a tree in which the probability distribution over words is decomposed into a product of two probabilities for each word, greatly reducing training and inference time as only the path specified by the hierarchy needs to be computed and updated", "question_type": "Shallow question", "evidential_info": [], "composition": "The paper does not detail the two probabilities in hierarchical softmax.", "arxiv_id": "1602.02410", "s2orc_url": "https://www.semanticscholar.org/paper/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "arxiv_url": "https://arxiv.org/abs/1602.02410"}, "133": {"paper_id": "paper_2", "title": "Exploring the Limits of Language Modeling", "question_id": 8, "question": "How does one choose a good hierarchy for a hierarchical softmax? ", "question_section": "3.1 Relationship between Noise Contrastive Estimation and Importance Sampling", "question_trigger_sentence": "Choosing a good hierarchy is important for obtaining good results and we did not explore this approach further for this paper as sampling methods worked well for our setup.", "question_type": "Shallow question", "evidential_info": [], "composition": "The paper does not discuss methods on choosing the hierarchies for hierarchical softmax.", "arxiv_id": "1602.02410", "s2orc_url": "https://www.semanticscholar.org/paper/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "arxiv_url": "https://arxiv.org/abs/1602.02410"}, "134": {"paper_id": "paper_2", "title": "Exploring the Limits of Language Modeling", "question_id": 9, "question": "What is a \"normalization term\" ", "question_section": "3.1 Relationship between Noise Contrastive Estimation and Importance Sampling", "question_trigger_sentence": "Sampling approaches are only useful during training, as they propose an approximation to the loss which is cheap to compute (also in a distributed setting) \u2013 however, at inference time one still has to compute the normalization term over all words.", "question_type": "Shallow question", "evidential_info": [], "composition": "The paper does not define normalization terms.", "arxiv_id": "1602.02410", "s2orc_url": "https://www.semanticscholar.org/paper/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "arxiv_url": "https://arxiv.org/abs/1602.02410"}, "135": {"paper_id": "paper_2", "title": "Exploring the Limits of Language Modeling", "question_id": 11, "question": "What is it that allows the combined character/word-level LSTM to become vocabulary size-agnostic? ", "question_section": "3.3 Char LSTM Predictions", "question_trigger_sentence": "The resulting model scales independently of vocabulary size \u2013 both for training and inference.", "question_type": "Shallow question", "evidential_info": [], "composition": "The paper does not discuss how charLSTM can perform independent of vocabulary size.", "arxiv_id": "1602.02410", "s2orc_url": "https://www.semanticscholar.org/paper/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "arxiv_url": "https://arxiv.org/abs/1602.02410"}, "136": {"paper_id": "paper_2", "title": "Exploring the Limits of Language Modeling", "question_id": 12, "question": "What does 'UNK' mean? 'Unknown?'", "question_section": "4.1 Data Set", "question_trigger_sentence": " The words that are out of vocabulary (OOV) are marked with a special UNK token (there are approximately 0.3% such words).", "question_type": "testing question", "evidential_info": [], "composition": "The paper does not define UNK.", "arxiv_id": "1602.02410", "s2orc_url": "https://www.semanticscholar.org/paper/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "arxiv_url": "https://arxiv.org/abs/1602.02410"}, "137": {"paper_id": "paper_2", "title": "Exploring the Limits of Language Modeling", "question_id": 13, "question": "Would padding impact accuracy of the model? ", "question_section": "4.2 Model Setup", "question_trigger_sentence": "Whenever a sentence ends, a new one starts without any padding (thus maximizing the occupancy per batch).", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The paper does not discuss the impacts of padding on performance.", "arxiv_id": "1602.02410", "s2orc_url": "https://www.semanticscholar.org/paper/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "arxiv_url": "https://arxiv.org/abs/1602.02410"}, "138": {"paper_id": "paper_2", "title": "Exploring the Limits of Language Modeling", "question_id": 15, "question": "What is the tradeoff of using a larger amount of samples per step? ", "question_section": "4.4 Training Procedure", "question_trigger_sentence": "We used a large number of negative (or noise) samples: 8192 such samples were drawn per step, but were shared across all the target words in the batch (2560 total, i.e. 128 times 20 unrolled steps). This results in multiplying (2560 x 1024) times (1024 x (8192+1)) (instead of (2560 x 1024) times (1024 x 793471)), i.e. about 100-fold less computation.", "question_type": "deep/complex question", "evidential_info": [], "composition": "The paper does not discuss the gains or losses incurred from larger samples per step.", "arxiv_id": "1602.02410", "s2orc_url": "https://www.semanticscholar.org/paper/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "arxiv_url": "https://arxiv.org/abs/1602.02410"}, "139": {"paper_id": "paper_2", "title": "Exploring the Limits of Language Modeling", "question_id": 14, "question": "What does SNM10-skip stand for? ", "question_section": "4.3 Model Architecture", "question_trigger_sentence": "10 LSTMs + SNM10-skip (Shazeer et al., 2015)\t23.7", "question_type": "Deep/complex question", "evidential_info": [], "composition": "It stands for Sparse Non-negative Matrix language modeling with skip connections.", "arxiv_id": "1602.02410", "s2orc_url": "https://www.semanticscholar.org/paper/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "arxiv_url": "https://arxiv.org/abs/1602.02410"}, "140": {"paper_id": "paper_24", "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms", "question_id": 17, "question": "What evaluation metrics have been used to benchmark the ML algorithms on the FashionMNIST dataset?", "question_section": "Experiments", "question_trigger_sentence": "We provide some classification results in Table 3 to form a benchmark on this data set.", "question_type": "Shallow question", "evidential_info": [], "composition": "Exactly which evaluation metrics has been used has not been mentioned in the paper.", "arxiv_id": "1708.07747", "s2orc_url": "https://www.semanticscholar.org/paper/f9c602cc436a9ea2f9e7db48c77d924e09ce3c32", "arxiv_url": "https://arxiv.org/abs/1708.07747"}, "141": {"paper_id": "paper_3", "title": "Self-Instruct: Aligning Language Model with Self Generated Instructions", "question_id": 14, "question": "What does \"ROUGE-L\" stand for?", "question_section": "4 SELF-INSTRUCT Data from GPT3", "question_trigger_sentence": "For each generated instruction, we compute its highest ROUGE-L overlap with the 175 seed instructions.", "question_type": "Testing question", "evidential_info": [], "composition": "The full form of ROGUE-L (if any) could not be found in this paper.", "arxiv_id": "2212.10560", "s2orc_url": "https://www.semanticscholar.org/paper/e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "arxiv_url": "https://arxiv.org/abs/2212.10560"}, "142": {"paper_id": "paper_30", "title": "Disentangling Disentanglement in Variational Autoencoders", "question_id": 2, "question": "What is the loss used to train VAE ?", "question_section": "2. Background and Related Work", "question_trigger_sentence": "Learning proceeds by minimising a divergence between the true data generating distribution and the model w.r.t \u03b8, typically arg min \u03b8 KL(pD(x) k p\u03b8(x)) = arg max \u03b8 EpD(x) [log p\u03b8(x)] where p\u03b8(x) = R Z p\u03b8(x|z)p(z)dz is the marginal likelihood, or evidence, of datapoint x under the model, approximated by averaging over the observations.", "question_type": "Shallow question", "evidential_info": [], "composition": "The loss used to train VAEs cannot be answered in this paper.", "arxiv_id": "1812.02833", "s2orc_url": "https://www.semanticscholar.org/paper/3f51216e1834c4fe06b08df87901ae0d77de2567", "arxiv_url": "https://arxiv.org/abs/1812.02833"}, "143": {"paper_id": "paper_30", "title": "Disentangling Disentanglement in Variational Autoencoders", "question_id": 10, "question": "How does increasing the value of \u03b2 in the VAE loss function impact the latent space?", "question_section": "6. Experiments", "question_trigger_sentence": "We see in Figure 3 that increasing \u03b2 increases the level of overlap in q\u03c6(z), as a consequence of increasing the encoder variance for individual datapoints. When \u03b2 is too large, the encoding of a datapoint loses meaning.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The impact of increasing the value of \u03b2 in the VAE loss function cannot be answered in this paper", "arxiv_id": "1812.02833", "s2orc_url": "https://www.semanticscholar.org/paper/3f51216e1834c4fe06b08df87901ae0d77de2567", "arxiv_url": "https://arxiv.org/abs/1812.02833"}, "144": {"paper_id": "paper_30", "title": "Disentangling Disentanglement in Variational Autoencoders", "question_id": 11, "question": "What is the effect of increasing the value of \u03b1 on the match between the prior and the approximate posterior in the VAE model?", "question_section": "6. Experiments", "question_trigger_sentence": "Increasing \u03b1, instead always improved the match between q\u03c6(z) and p(z). Here, the finiteness of the dataset and the choice of divergence results in an increase in overlap with increasing \u03b1, but only up to the level required for a non-negligible overlap between the nearby datapoints: large values of \u03b1 did not cause the encodings to collapse to a mode.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The effect of increasing the value of \u03b1 on the match between the prior and the approximate posterior in the VAE model cannot be answered in this paper", "arxiv_id": "1812.02833", "s2orc_url": "https://www.semanticscholar.org/paper/3f51216e1834c4fe06b08df87901ae0d77de2567", "arxiv_url": "https://arxiv.org/abs/1812.02833"}, "145": {"paper_id": "paper_30", "title": "Disentangling Disentanglement in Variational Autoencoders", "question_id": 12, "question": "How does the degree of sparsity in the latent space affect the match between the prior and the approximate posterior in the VAE model?", "question_section": "6. Experiments", "question_trigger_sentence": "Increasing the degree of sparsity in the latent space improved the match between the prior and the approximate posterior", "question_type": "Deep/complex question", "evidential_info": [], "composition": "the way in which degree of sparsity in the latent space affect the match between the prior and the approximate posterior in the VAE model cannot be answered in this paper", "arxiv_id": "1812.02833", "s2orc_url": "https://www.semanticscholar.org/paper/3f51216e1834c4fe06b08df87901ae0d77de2567", "arxiv_url": "https://arxiv.org/abs/1812.02833"}, "146": {"paper_id": "paper_35", "title": "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation", "question_id": 8, "question": "What does \"Markov random fields\" mean ?", "question_section": "Introduction and Related Work", "question_trigger_sentence": "Post-processing approaches such as connected components analysis normally yield no improvement and therefore, more recent works, propose to use the network predictions in combination with Markov random fields, voting strategies or more traditional approaches such as level-sets", "question_type": "Testing question", "evidential_info": [], "composition": "The process of Markov random fields can not be explained from this paper.", "arxiv_id": "1606.04797", "s2orc_url": "https://www.semanticscholar.org/paper/50004c086ffd6a201a4b782281aaa930fbfe6ecf", "arxiv_url": "https://arxiv.org/abs/1606.04797"}, "147": {"paper_id": "paper_35", "title": "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation", "question_id": 14, "question": "Can we add regularization terms to dice loss ?", "question_section": "Dice loss layer", "question_trigger_sentence": "Several previous approaches resorted to loss functions based on sample re-weighting where foreground regions are given more importance than background ones during learning. In this work we propose a novel objective function based on dice coefficient, which is a quantity ranging between 0 and 1 which we aim to maximise.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The paper does not have the answer of whether we can add regularization terms to Dice loss or not.", "arxiv_id": "1606.04797", "s2orc_url": "https://www.semanticscholar.org/paper/50004c086ffd6a201a4b782281aaa930fbfe6ecf", "arxiv_url": "https://arxiv.org/abs/1606.04797"}, "148": {"paper_id": "paper_35", "title": "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation", "question_id": 17, "question": "What is Imorphics model advantage over the proposed model V-net?", "question_section": "Results", "question_trigger_sentence": "Table 2. Quantitative comparison between the proposed approach and the current best results on the PROMISE 2012 challenge dataset.", "question_type": "Testing question", "evidential_info": [], "composition": "The paper does not provide any advantages of Imorphics model over the proposed model.", "arxiv_id": "1606.04797", "s2orc_url": "https://www.semanticscholar.org/paper/50004c086ffd6a201a4b782281aaa930fbfe6ecf", "arxiv_url": "https://arxiv.org/abs/1606.04797"}, "149": {"paper_id": "paper_35", "title": "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation", "question_id": 22, "question": "Where does the discouraging of the use of max-pooling operations in CNNs arise from?", "question_section": "Method", "question_trigger_sentence": "This strategy serves a similar purpose as pooling layers that, motivated by and other works discouraging the use of max-pooling operations in CNNs, have been replaced in our approach by convolutional ones", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Pooling operations with convolutional ones results also to networks that, depending on the specific implementation, can have a smaller memory footprint during training.", "arxiv_id": "1606.04797", "s2orc_url": "https://www.semanticscholar.org/paper/50004c086ffd6a201a4b782281aaa930fbfe6ecf", "arxiv_url": "https://arxiv.org/abs/1606.04797"}, "150": {"paper_id": "paper_35", "title": "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation", "question_id": 23, "question": "What if the augmented data is still not enough to obtain high reliable performance?", "question_section": "Training", "question_trigger_sentence": "In this work we found necessary to augment the original training dataset in order to obtain robustness and increased precision on the test dataset.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The paper does not discuss the performance with less augmented data.", "arxiv_id": "1606.04797", "s2orc_url": "https://www.semanticscholar.org/paper/50004c086ffd6a201a4b782281aaa930fbfe6ecf", "arxiv_url": "https://arxiv.org/abs/1606.04797"}, "151": {"paper_id": "paper_35", "title": "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation", "question_id": 24, "question": "Is using unpooling operations of same difficulty as using deconvolutional operations?", "question_section": "Method", "question_trigger_sentence": "Replacing pooling operations with convolutional ones results also to networks that, depending on the specific implementation, can have a smaller memory footprint during training, due to the fact that no switches mapping the output of pooling layers back to their inputs are needed for back-propagation, and that can be better understood and analyzed by applying only de-convolutions instead of unpooling operations.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The paper does not compare the complexity of unpooling and deconvolution operations.", "arxiv_id": "1606.04797", "s2orc_url": "https://www.semanticscholar.org/paper/50004c086ffd6a201a4b782281aaa930fbfe6ecf", "arxiv_url": "https://arxiv.org/abs/1606.04797"}, "152": {"paper_id": "paper_35", "title": "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation", "question_id": 25, "question": " what would be the total effect of adding regularization terms to dice loss?", "question_section": "Dice loss layer", "question_trigger_sentence": "Several previous approaches resorted to loss functions based on sample re-weighting where foreground regions are given more importance than background ones during learning. In this work we propose a novel objective function based on dice coefficient, which is a quantity ranging between 0 and 1 which we aim to maximise.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The paper does not discuss any regularizing terms in the dice loss function.", "arxiv_id": "1606.04797", "s2orc_url": "https://www.semanticscholar.org/paper/50004c086ffd6a201a4b782281aaa930fbfe6ecf", "arxiv_url": "https://arxiv.org/abs/1606.04797"}, "153": {"paper_id": "paper_35", "title": "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation", "question_id": 15, "question": "How could we use statistical techniques analogous to density estimation to handle the lack in MRI data?", "question_section": "Training", "question_trigger_sentence": "In this work we found necessary to augment the original training dataset in order to obtain robustness and increased precision on the test dataset.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The paper does not discuss the effect of batch norm layer on weight initializations.", "arxiv_id": "1606.04797", "s2orc_url": "https://www.semanticscholar.org/paper/50004c086ffd6a201a4b782281aaa930fbfe6ecf", "arxiv_url": "https://arxiv.org/abs/1606.04797"}, "154": {"paper_id": "paper_36", "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "question_id": 15, "question": "Is it true that adding a batch norm layer  overcomes the problem mentioned here \"a good initialization of the weights is extremely important. Otherwise, parts of the network might give excessive activations, while other parts never contribute. \"", "question_section": "Training", "question_trigger_sentence": "Ideally the initial weights should be adapted such that each feature map in the network has approximately unit variance.", "question_type": "Shallow question", "evidential_info": [], "composition": "The paper does not discuss the effect of batch norm layer on weight initializations.", "arxiv_id": "1505.04597", "s2orc_url": "https://www.semanticscholar.org/paper/6364fdaa0a0eccd823a779fcdd489173f938e91a", "arxiv_url": "https://arxiv.org/abs/1505.04597"}, "155": {"paper_id": "paper_36", "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "question_id": 18, "question": "Does Dropout resembles ensembling techniques?", "question_section": "Data Augmentation", "question_trigger_sentence": "Drop-out layers at the end of the contracting path perform further implicit data augmentation.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The paper does not discuss ensembling and dropout layer comparisons.", "arxiv_id": "1505.04597", "s2orc_url": "https://www.semanticscholar.org/paper/6364fdaa0a0eccd823a779fcdd489173f938e91a", "arxiv_url": "https://arxiv.org/abs/1505.04597"}, "156": {"paper_id": "paper_36", "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "question_id": 6, "question": "How is the model performance compared to the fully connected network in detecting images", "question_section": "Introduction", "question_trigger_sentence": "The network does not have any fully connected layers and only uses the valid part of each convolution, i.e., the segmentation map only contains the pixels, for which the full context is available in the input image", "question_type": "Testing question", "evidential_info": [], "composition": "Paper does not show comparison of performance between U-Net and fully connected network.", "arxiv_id": "1505.04597", "s2orc_url": "https://www.semanticscholar.org/paper/6364fdaa0a0eccd823a779fcdd489173f938e91a", "arxiv_url": "https://arxiv.org/abs/1505.04597"}, "157": {"paper_id": "paper_37", "title": "R-FCN: Object Detection via Region-based Fully Convolutional Networks", "question_id": 21, "question": "What do  results of comparisons between R-FCN and YOLO architecture tell us?", "question_section": "Experiments: Comparisons with Other Fully Convolutional Strategies", "question_trigger_sentence": "Though fully convolutional detectors are available, experiments show that it is nontrivial for them to achieve good accuracy. We investigate the following fully convolutional strategies (or \u201calmost\u201d fully convolutional strategies that have only one classifier fc layer per RoI), using ResNet-101", "question_type": "Deep/complex question", "evidential_info": [], "composition": "This paper does not mention YOLO and does not compare R-FCN with YOLO, so the question is unanswerable.", "arxiv_id": "1605.06409", "s2orc_url": "https://www.semanticscholar.org/paper/b724c3f7ff395235b62537203ddeb710f0eb27bb", "arxiv_url": "https://arxiv.org/abs/1605.06409"}, "158": {"paper_id": "paper_37", "title": "R-FCN: Object Detection via Region-based Fully Convolutional Networks", "question_id": 11, "question": "Should it be more parameter saving if we use only center of box,height, width instead of (tx, ty , tw, th) in bounding box regression ?", "question_section": "Our approach: Position-sensitive score maps & Position-sensitive RoI pooling.", "question_trigger_sentence": " This 4-d vector parameterizes a bounding box as t = (tx, ty , tw, th) following the parameterization in [6]. ", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The authors do not discuss the possibility of changing the parameterization, but it seems that the asker is correct in that their parameterization would reduce the number of parameters.", "arxiv_id": "1605.06409", "s2orc_url": "https://www.semanticscholar.org/paper/b724c3f7ff395235b62537203ddeb710f0eb27bb", "arxiv_url": "https://arxiv.org/abs/1605.06409"}, "159": {"paper_id": "paper_37", "title": "R-FCN: Object Detection via Region-based Fully Convolutional Networks", "question_id": 7, "question": "What are the advantages of removing global average pooling layer from R-FCN architecture?", "question_section": "Our aproach: Backbone architecture", "question_trigger_sentence": " ResNet-101 has 100 convolutional layers followed by global average pooling and a 1000-class fc layer. We remove the average pooling layer and the fc layer and only use the convolutional layers to compute feature maps.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The authors do not mention any reasons for removing the average pooling layer.", "arxiv_id": "1605.06409", "s2orc_url": "https://www.semanticscholar.org/paper/b724c3f7ff395235b62537203ddeb710f0eb27bb", "arxiv_url": "https://arxiv.org/abs/1605.06409"}, "160": {"paper_id": "paper_37", "title": "R-FCN: Object Detection via Region-based Fully Convolutional Networks", "question_id": 1, "question": "What is the difference in output nature between the mentioned sub-networks based on RoIs?", "question_section": "Introduction", "question_trigger_sentence": "A prevalent family of deep networks for object detection can be divided into two subnetworks by the Region-of-Interest (RoI) pooling layer [ 6]: (i) a shared, \u201cfully convolutional\u201d subnetwork independent of RoIs, and (ii) an RoI-wise subnetwork that does not share computation. ", "question_type": "Testing question", "evidential_info": [], "composition": "The authors do not mention whether there is a difference in the nature of the output of the different subnetworks.", "arxiv_id": "1605.06409", "s2orc_url": "https://www.semanticscholar.org/paper/b724c3f7ff395235b62537203ddeb710f0eb27bb", "arxiv_url": "https://arxiv.org/abs/1605.06409"}, "161": {"paper_id": "paper_37", "title": "R-FCN: Object Detection via Region-based Fully Convolutional Networks", "question_id": 0, "question": "Is classification done simultaneously across the entire input map while localization?", "question_section": "Abstract", "question_trigger_sentence": "In contrast to previous region-based detectors such as Fast/Faster R-CNN  that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image.", "question_type": "Shallow question", "evidential_info": [], "composition": "The authors do not mention localization, so the question is unanswerable.", "arxiv_id": "1605.06409", "s2orc_url": "https://www.semanticscholar.org/paper/b724c3f7ff395235b62537203ddeb710f0eb27bb", "arxiv_url": "https://arxiv.org/abs/1605.06409"}, "162": {"paper_id": "paper_4", "title": "Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space", "question_id": 9, "question": "Since NLI datasets are known to have artifacts and biases, could this finding imply that Optimus can also be susceptible to propagate biases in datasets more?", "question_section": "Experimental Results", "question_trigger_sentence": "This gap is larger, when the sentences in the dataset exhibit common regularities, such as SNLI, where the prior plays a more important/effective role in this scenario. ", "question_type": "Deep/complex question", "evidential_info": [], "composition": "This question cannot be answered from information contained in this paper, since the authors do not discuss the impact of bias in the datasets they use.", "arxiv_id": "2004.04092", "s2orc_url": "https://www.semanticscholar.org/paper/00696ba295d66f049d70272219f7fea4266171be", "arxiv_url": "https://arxiv.org/abs/2004.04092"}, "163": {"paper_id": "paper_4", "title": "Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space", "question_id": 16, "question": "What is the difference between this work and SentenceTransformers, which are also able to encode sentences in a feature space?", "question_section": "Related Work", "question_trigger_sentence": "However, all of these models do not generally learn a smooth, interpretable feature space for sentence encoding, or generating novel sentences", "question_type": "Testing question", "evidential_info": [], "composition": "This paper does not compare the proposed approach, OPTIMUS, to sentence transformers, so it is not possible to explain the differences between these two without additional context or information.", "arxiv_id": "2004.04092", "s2orc_url": "https://www.semanticscholar.org/paper/00696ba295d66f049d70272219f7fea4266171be", "arxiv_url": "https://arxiv.org/abs/2004.04092"}, "164": {"paper_id": "paper_4", "title": "Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space", "question_id": 19, "question": "What is the re-parametrization trick?", "question_section": "Pre-trained Latent Space Modeling", "question_trigger_sentence": "Typically,  is modeled as a Gaussian distribution, and the re-parametrization trick is used for efficient learning Kingma and Welling (2013).", "question_type": "Shallow question", "evidential_info": [], "composition": "There is no explanation for what the re-parameterization trick is in this paper.", "arxiv_id": "2004.04092", "s2orc_url": "https://www.semanticscholar.org/paper/00696ba295d66f049d70272219f7fea4266171be", "arxiv_url": "https://arxiv.org/abs/2004.04092"}, "165": {"paper_id": "paper_4", "title": "Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space", "question_id": 13, "question": "Why do existing models like BART and T5 lack explicit modeling of structures in a latent space if they are able to produce output embedding predictions?", "question_section": "Introduction", "question_trigger_sentence": "Although significant performance improvement has been reported on a wide range of NLP tasks, these models lack of explicit modeling of structures in a latent space, rendering it difficult to control natural language generation/representation from an abstract level.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "This paper does not contain information on why models such as T5 or BART are unable to model structures in a latent space.", "arxiv_id": "2004.04092", "s2orc_url": "https://www.semanticscholar.org/paper/00696ba295d66f049d70272219f7fea4266171be", "arxiv_url": "https://arxiv.org/abs/2004.04092"}, "166": {"paper_id": "paper_41", "title": "Attention Is All You Need", "question_id": 23, "question": "What about learning parallel tasks?(i.e., learning similarities, contexts and useful representations from features or outputs conducted by the teacher network to perform other tasks with the student network)", "question_section": "Conclusion", "question_trigger_sentence": "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Use of transformers for the parallel tasks involving teacher and student networks have not been discussed in the paper,", "arxiv_id": "1706.03762", "s2orc_url": "https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "arxiv_url": "https://arxiv.org/abs/1706.03762"}, "167": {"paper_id": "paper_41", "title": "Attention Is All You Need", "question_id": 25, "question": "Could we use distillation technique in training and learning of the transformer? could this approach make it easier while inference or faster?", "question_section": "Conclusion", "question_trigger_sentence": "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The paper does not discuss the effects of using the distillation technique for training the transformer.", "arxiv_id": "1706.03762", "s2orc_url": "https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "arxiv_url": "https://arxiv.org/abs/1706.03762"}, "168": {"paper_id": "paper_41", "title": "Attention Is All You Need", "question_id": 13, "question": "What would be the result if we apply kernel tricks or nonlinearities to Q,V,K before taking scaled dot products? should this capture nonlinear attention or relations between them somehow?", "question_section": "Scaled Dot-Product Attention", "question_trigger_sentence": "We call our particular attention \"Scaled Dot-Product Attention\"). The input consists of queries and keys of dimension dk, and values of dimension dv . We compute the dot products of the query with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the values", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The paper does not discuss the effect of kernel tricks on the Q, V and K.", "arxiv_id": "1706.03762", "s2orc_url": "https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "arxiv_url": "https://arxiv.org/abs/1706.03762"}, "169": {"paper_id": "paper_42", "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications", "question_id": 17, "question": "How and why you might need to make MobileNet shallower.", "question_section": "Experiments", "question_trigger_sentence": "To make MobileNet shallower", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Authors\u2019 didn\u2019t discuss the reasons or procedures of making a model shallower.", "arxiv_id": "1704.04861", "s2orc_url": "https://www.semanticscholar.org/paper/3647d6d0f151dc05626449ee09cc7bce55be497e", "arxiv_url": "https://arxiv.org/abs/1704.04861"}, "170": {"paper_id": "paper_42", "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications", "question_id": 18, "question": "What task MobileNet showed better results in.", "question_section": "Experiments", "question_trigger_sentence": "We train MobileNet for fine grained recognition on the", "question_type": "Shallow question", "evidential_info": [], "composition": "Paragraphs\u2019 don't discuss or compares the tasks at which MobileNet betters.", "arxiv_id": "1704.04861", "s2orc_url": "https://www.semanticscholar.org/paper/3647d6d0f151dc05626449ee09cc7bce55be497e", "arxiv_url": "https://arxiv.org/abs/1704.04861"}, "171": {"paper_id": "paper_44", "title": "Accurate Image Super-Resolution Using Very Deep Convolutional Networks", "question_id": 21, "question": "Why did the authors augmented the data ?", "question_section": "Experimental Results", "question_trigger_sentence": "In addition, data augmentation (rotation or flip) is used", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The authors do not provide any explanation for why they thought it was necessary to do data augmentation.", "arxiv_id": "1511.04587", "s2orc_url": "https://www.semanticscholar.org/paper/b5f3e5d2912bedbcd9458952d664b08db6aed962", "arxiv_url": "https://arxiv.org/abs/1511.04587"}, "172": {"paper_id": "paper_49", "title": "Universal Language Model Fine-tuning for Text Classification", "question_id": 2, "question": "\u201cCompared to CV, NLP models are typically more shallow and thus require different fine-tuning methods.\u201d Does this hold true today, with the emergence of models such as GPT3 and DALLE-2 after this paper was published?", "question_section": "1 Introduction", "question_trigger_sentence": "Compared to CV, NLP models are typically more shallow and thus require different fine-tuning methods.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Whether NLP models being more shallow and requiring different fine-tuning methods still holds with recent models such as GPT3 and DALLE-2 cannot be answered with this paper.", "arxiv_id": "1801.06146", "s2orc_url": "https://www.semanticscholar.org/paper/ad76c236fe641aa52d1d6c28bf362ae9ffac91e7", "arxiv_url": "https://arxiv.org/abs/1801.06146"}, "173": {"paper_id": "paper_5", "title": "MemPrompt: Memory-assisted Prompt Editing with User Feedback", "question_id": 11, "question": "Would a similar approach work on long-form generation tasks and on more open-ended (or creative) tasks?", "question_section": "Approach", "question_trigger_sentence": "Further, unlike long-form generation tasks, we can fit a large number of samples in the prompt, providing a strong baseline that is used to compare our approach with.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "From the paper, there is no information that would help answer the question on whether this approach (MemPrompt) would work on long-form creative, generative tasks.", "arxiv_id": "2201.06009", "s2orc_url": "https://www.semanticscholar.org/paper/41f44979cf1cd3f4cbd615dc130bc33721f5281b", "arxiv_url": "https://arxiv.org/abs/2201.06009"}, "174": {"paper_id": "paper_5", "title": "MemPrompt: Memory-assisted Prompt Editing with User Feedback", "question_id": 10, "question": "How do the authors control the evaluation method when the quality of the prompts can largely affect the accuracy of GPT3?", "question_section": "Introduction", "question_trigger_sentence": "In a constrained setting, this implementation doubles GPT3\u2019s accuracy on basic lexical tasks (e.g., generate a synonym) using simulated feedback, without retraining.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The authors of Memprompt do not discuss how they control the evaluation method to take into account the variable quality of prompts that a user might input.", "arxiv_id": "2201.06009", "s2orc_url": "https://www.semanticscholar.org/paper/41f44979cf1cd3f4cbd615dc130bc33721f5281b", "arxiv_url": "https://arxiv.org/abs/2201.06009"}, "175": {"paper_id": "paper_50", "title": "Teaching Machines to Read and Comprehend", "question_id": 8, "question": "What does \"semantic parsing\" mean?", "question_section": "3.1 Symbolic Matching Models", "question_trigger_sentence": "Traditionally, a pipeline of NLP models has been used for attempting question answering, that is models that make heavy use of linguistic annotation, structured world knowledge and semantic parsing and similar NLP pipeline outputs. Building on these approaches, we define a number of NLP-centric models for our machine reading task.", "question_type": "Testing question", "evidential_info": [], "composition": "This paper does not formally define what semantic parsing means.", "arxiv_id": "1506.03340", "s2orc_url": "https://www.semanticscholar.org/paper/d1505c6123c102e53eb19dff312cb25cea840b72", "arxiv_url": "https://arxiv.org/abs/1506.03340"}, "176": {"paper_id": "paper_50", "title": "Teaching Machines to Read and Comprehend", "question_id": 9, "question": "What is POS tagging?", "question_section": "3.2 Neural Network Models", "question_trigger_sentence": "Neural networks have successfully been applied to a range of tasks in NLP. This includes classification tasks such as sentiment analysis [15] or POS tagging [16]", "question_type": "Testing question", "evidential_info": [], "composition": "This paper refers to POS tagging in passing, but does not explain what it means.", "arxiv_id": "1506.03340", "s2orc_url": "https://www.semanticscholar.org/paper/d1505c6123c102e53eb19dff312cb25cea840b72", "arxiv_url": "https://arxiv.org/abs/1506.03340"}, "177": {"paper_id": "paper_50", "title": "Teaching Machines to Read and Comprehend", "question_id": 15, "question": "The authors used English-language-only sources to create their dataset. Is it likely that their symbolic matching models would perform the same way in languages whose grammar and sentence structure is very different? ", "question_section": "3.1 Symbolic Matching Models", "question_trigger_sentence": "Frame-Semantic Parsing Frame-semantic parsing attempts to identify predicates and their arguments, allowing models access to information about \u201cwho did what to whom\u201d. Naturally this kind of annotation lends itself to being exploited for question answering", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The answer to this question, of whether symbolic matching models would have similar performance in non-English languages with varying sentence and grammar structures, cannot be answered through the information in this paper alone.", "arxiv_id": "1506.03340", "s2orc_url": "https://www.semanticscholar.org/paper/d1505c6123c102e53eb19dff312cb25cea840b72", "arxiv_url": "https://arxiv.org/abs/1506.03340"}, "178": {"paper_id": "paper_50", "title": "Teaching Machines to Read and Comprehend", "question_id": 21, "question": "What are Memory Network?", "question_section": "3.2 Neural Network Models", "question_trigger_sentence": "The Attentive Reader can be viewed as a generalisation of the application of Memory Networks to question answering [3].", "question_type": "Testing question", "evidential_info": [], "composition": "The definition of memory networks is not in this paper.", "arxiv_id": "1506.03340", "s2orc_url": "https://www.semanticscholar.org/paper/d1505c6123c102e53eb19dff312cb25cea840b72", "arxiv_url": "https://arxiv.org/abs/1506.03340"}, "179": {"paper_id": "paper_50", "title": "Teaching Machines to Read and Comprehend", "question_id": 17, "question": "Why is the correct frame strategy given a higher priority than the permuted framstrategy?", "question_section": "3.1 Symbolic Matching Models", "question_trigger_sentence": "Table 4: Resolution strategies using PropBank triples. x denotes the entity proposed as answer, V is a fully qualified PropBank frame (e.g. give.01.V). Strategies are ordered by precedence and answers determined accordingly. This heuristic algorithm was iteratively tuned on the validation data set.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The rationale on why this specific ordering of strategies, correct frame having higher priority than permuted frame, is used is not explicitly explained in this paper.", "arxiv_id": "1506.03340", "s2orc_url": "https://www.semanticscholar.org/paper/d1505c6123c102e53eb19dff312cb25cea840b72", "arxiv_url": "https://arxiv.org/abs/1506.03340"}, "180": {"paper_id": "paper_51", "title": "Show and tell: A neural image caption generator", "question_id": 1, "question": "Would the authors' proposed model still reach convergence if it were to be trained with AdamW or Adam instead of SGD?", "question_section": "1. Introduction", "question_trigger_sentence": "It is a neural net which is fully trainable using stochastic gradient descent.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "As there is no evidential information about how the given optimization method (stochastic gradient descent/AdamW/Adam) works, this question cannot be answered in this paper and requires external knowledge.", "arxiv_id": "1411.4555", "s2orc_url": "https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "arxiv_url": "https://arxiv.org/abs/1411.4555"}, "181": {"paper_id": "paper_52", "title": "MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems", "question_id": 1, "question": "Give an example of a ternary DL operator (i.e. three operands).", "question_section": "2.1 Symbol: Declarative Symbolic Expressions", "question_trigger_sentence": "Symbols are composited by operators, such as simple matrix operations (e.g. \u201c+\u201d), or a complex neural network layer (e.g. convolution layer).", "question_type": "Testing question", "evidential_info": [], "composition": "This question cannot be answered since there is no evidential information about the DL operator, thus external knowledge is required.", "arxiv_id": "1512.01274", "s2orc_url": "https://www.semanticscholar.org/paper/62df84d6a4d26f95e4714796c2337c9848cc13b5", "arxiv_url": "https://arxiv.org/abs/1512.01274"}, "182": {"paper_id": "paper_52", "title": "MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems", "question_id": 4, "question": "The paper mentions that MXNet supports both sequential and eventual consistency. What is the difference between these two?", "question_section": "2.3 KVStore: Data Synchronization Over Devices", "question_trigger_sentence": "Currently, we support the sequential and eventual consistency.", "question_type": "Testing question", "evidential_info": [], "composition": "Since there are no evidential information about sequential and eventual consistency, this question cannot be answered and requires external knowledges.", "arxiv_id": "1512.01274", "s2orc_url": "https://www.semanticscholar.org/paper/62df84d6a4d26f95e4714796c2337c9848cc13b5", "arxiv_url": "https://arxiv.org/abs/1512.01274"}, "183": {"paper_id": "paper_53", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks", "question_id": 4, "question": "How many samples are there in the CIFAR dataset?", "question_section": "3.2. Linear Bottlenecks", "question_trigger_sentence": "We note that similar reports where non-linearity was helped were reported in [29] where non-linearity was removed from the input of the traditional residual block and that lead to improved performance on CIFAR dataset.", "question_type": "Testing question", "evidential_info": [], "composition": "This question cannot be answered since there is no evidential information for the CIFAR dataset in the paper and requires external knowledges.", "arxiv_id": "1801.04381", "s2orc_url": "https://www.semanticscholar.org/paper/dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "arxiv_url": "https://arxiv.org/abs/1801.04381"}, "184": {"paper_id": "paper_58", "title": "Graph Attention Networks", "question_id": 14, "question": "What does \u201cinductive learning\u201dmean?", "question_section": "GAT structure", "question_trigger_sentence": "It makes our technique directly applicable to inductive learning\u2014including tasks where the model is evaluated on graphs that are completely unseen during training.", "question_type": "Testing question", "evidential_info": [], "composition": "There is no strict definition or clue to find out the meaning of \"inductive learning\". Only several benchmarks were provided.", "arxiv_id": "1710.10903", "s2orc_url": "https://www.semanticscholar.org/paper/33998aff64ce51df8dee45989cdca4b6b1329ec4", "arxiv_url": "https://arxiv.org/abs/1710.10903"}, "185": {"paper_id": "paper_59", "title": "Fully convolutional networks for semantic segmentation", "question_id": 0, "question": "What was the problem of convnets for semantic segmentation?", "question_section": "Introduction", "question_trigger_sentence": "Prior approaches have used convnets for semantic segmentation [27, 2, 8, 28, 16, 14, 11], in which each pixel is labeled with the class of its enclosing object or region, but with shortcomings that this work addresses", "question_type": "Shallow question", "evidential_info": [], "composition": "There are no clue to find the problem of convnets for semantic segmentation.", "arxiv_id": "1411.4038", "s2orc_url": "https://www.semanticscholar.org/paper/6fc6803df5f9ae505cae5b2f178ade4062c768d0", "arxiv_url": "https://arxiv.org/abs/1411.4038"}, "186": {"paper_id": "paper_59", "title": "Fully convolutional networks for semantic segmentation", "question_id": 3, "question": "Did they mention a future work for this research topic?", "question_section": "Introduction", "question_trigger_sentence": "Unlike these existing methods, we adapt and extend deep classification architectures, using image classification as supervised pre-training, and fine-tune fully convolutionally to learn simply and efficiently from whole image inputs and whole image ground thruths.", "question_type": "Shallow question", "evidential_info": [], "composition": "No. They did not mentioned.", "arxiv_id": "1411.4038", "s2orc_url": "https://www.semanticscholar.org/paper/6fc6803df5f9ae505cae5b2f178ade4062c768d0", "arxiv_url": "https://arxiv.org/abs/1411.4038"}, "187": {"paper_id": "paper_6", "title": "Human Guided Exploitation of Interpretable Attention Patterns in Summarization and Topic Segmentation", "question_id": 12, "question": "Since the head to which the pattern has global relevance is found in the previous step, why can't the pattern be applied to that identified head?", "question_section": "Proposed Generic Pipeline", "question_trigger_sentence": "In the first scenario, although patterns can be directly injected into the pretrained encoder, determining to which heads the patterns should be applied to requires extensive hyperparameter search and risk overfitting.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The paper does not discuss whether a pattern can be applied to the head on which it was found.", "arxiv_id": "2112.05364", "s2orc_url": "https://www.semanticscholar.org/paper/571469045c49877f78a4522f7f21e8c30e2f5c89", "arxiv_url": "https://arxiv.org/abs/2112.05364"}, "188": {"paper_id": "paper_60", "title": "Fast R-CNN", "question_id": 7, "question": "How did author address the problem of high computational complexity caused by multi-stage pipeline?", "question_section": "Introduction", "question_trigger_sentence": "SPPnet also has notable drawbacks. Like R-CNN, training is a multi-stage pipeline that involves extracting features, fine-tuning a network with log loss, training SVMs, and finally fitting bounding-box regressors.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "less clue provided.", "arxiv_id": "1504.08083", "s2orc_url": "https://www.semanticscholar.org/paper/7ffdbc358b63378f07311e883dddacc9faeeaf4b", "arxiv_url": "https://arxiv.org/abs/1504.08083"}, "189": {"paper_id": "paper_61", "title": "Character-Aware Neural Language Models", "question_id": 6, "question": "What\u2019s a difference between data-L and data-S?", "question_section": "Results", "question_trigger_sentence": "Due to memory constraints we only train the small models on DATA-L (Table 5).", "question_type": "Testing question", "evidential_info": [], "composition": "Data-s has small size, while data-L has large size.", "arxiv_id": "1508.06615", "s2orc_url": "https://www.semanticscholar.org/paper/891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "arxiv_url": "https://arxiv.org/abs/1508.06615"}, "190": {"paper_id": "paper_61", "title": "Character-Aware Neural Language Models", "question_id": 7, "question": "What percentage of the data was used as a test set?", "question_section": "Experimental setting", "question_trigger_sentence": "We test the model on corpora of varying languages and sizes (statistics available in Table 1).", "question_type": "Testing question", "evidential_info": [], "composition": "There is no information about data percentage.", "arxiv_id": "1508.06615", "s2orc_url": "https://www.semanticscholar.org/paper/891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "arxiv_url": "https://arxiv.org/abs/1508.06615"}, "191": {"paper_id": "paper_64", "title": "Modeling Relational Data with Graph Convolutional Networks", "question_id": 0, "question": "Did the author apply RGCN to graph classification task as well?", "question_section": "abstract", "question_trigger_sentence": "We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes)", "question_type": "Shallow question", "evidential_info": [], "composition": "In this paper, the authors do not apply RGCN to graph classification task.\nTo perform graph classification, there is a need to generate graph representation with aggregating node representations using a readout function.\nThere is no such thing in this paper.\nThere is a dataset called MUTAG, which is widely used in graph classification.\nHowever, in this paper, entity classification was performed using a different MUTAG, which has the same name as the dataset.", "arxiv_id": "1703.06103", "s2orc_url": "https://www.semanticscholar.org/paper/cd8a9914d50b0ac63315872530274d158d6aff09", "arxiv_url": "https://arxiv.org/abs/1703.06103"}, "192": {"paper_id": "paper_70", "title": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks", "question_id": 12, "question": "Are there any techniques to reduce model oscillation other than using a history of generated images?", "question_section": "4. Implementation", "question_trigger_sentence": "Second, to reduce model oscillation [15], we follow Shrivastava et al.\u2019s strategy [46] and update the discriminators using a history of generated images rather than the ones produced by the latest generators. ", "question_type": "Shallow Question", "evidential_info": [], "composition": "[The paper doesn't mention any technique to reduce model oscillation other than using a history of generated images.]", "arxiv_id": "1703.10593", "s2orc_url": "https://www.semanticscholar.org/paper/c43d954cf8133e6254499f3d68e45218067e4941", "arxiv_url": "https://arxiv.org/abs/1703.10593"}, "193": {"paper_id": "paper_70", "title": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks", "question_id": 17, "question": "What is the definition of 'Gram Matrix'?", "question_section": "5.2. Applications", "question_trigger_sentence": "To compare against neural style transfer of an entire collection, we compute the average Gram Matrix across the target domain and use this matrix to transfer the \u201caverage style\u201d with Gatys et al [13].", "question_type": "Testing Question", "evidential_info": [], "composition": "[The paper uses \"Gram Matrix\", but doesn't define it.]", "arxiv_id": "1703.10593", "s2orc_url": "https://www.semanticscholar.org/paper/c43d954cf8133e6254499f3d68e45218067e4941", "arxiv_url": "https://arxiv.org/abs/1703.10593"}, "194": {"paper_id": "paper_70", "title": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks", "question_id": 8, "question": "Is this sentence true?", "question_section": "3.2. Cycle Consistency Loss", "question_trigger_sentence": "Adversarial training can, in theory, learn mappings G and F that produce outputs identically distributed as target domains Y and X respectively (strictly speaking, this re- quires G and F to be stochastic functions) [15].", "question_type": "Shallow Question", "evidential_info": [], "composition": "[ The question only asks \"Is this sentence true?\" but doesn't state any sentence. Therefore, the question cannot be answered.]", "arxiv_id": "1703.10593", "s2orc_url": "https://www.semanticscholar.org/paper/c43d954cf8133e6254499f3d68e45218067e4941", "arxiv_url": "https://arxiv.org/abs/1703.10593"}, "195": {"paper_id": "paper_72", "title": "Prototypical Networks for Few-shot Learning", "question_id": 0, "question": "What is the definition of 'inductive bias'?", "question_section": "Abstract", "question_trigger_sentence": "Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results.", "question_type": "Testing Question", "evidential_info": [], "composition": "[The paper uses \"inductive bias\" but doesn't define it anywhere in the paper.]", "arxiv_id": "1703.05175", "s2orc_url": "https://www.semanticscholar.org/paper/c269858a7bb34e8350f2442ccf37797856ae9bca", "arxiv_url": "https://arxiv.org/abs/1703.05175"}, "196": {"paper_id": "paper_72", "title": "Prototypical Networks for Few-shot Learning", "question_id": 5, "question": "What is the definition of convex function?", "question_section": "2.3 Prototypical Networks as Mixture Density Estimation", "question_trigger_sentence": "A regular Bregman divergence d\u03c6 is defined as:\nd\u03c6(z, z\u2032) = \u03c6(z) \u2212 \u03c6(z\u2032) \u2212 (z \u2212 z\u2032)T \u2207\u03c6(z\u2032), (3) where \u03c6 is a differentiable, strictly convex function of the Legendre type.", "question_type": "Testing Question", "evidential_info": [], "composition": "[The paper uses a \"convex function\", but does not define it.]", "arxiv_id": "1703.05175", "s2orc_url": "https://www.semanticscholar.org/paper/c269858a7bb34e8350f2442ccf37797856ae9bca", "arxiv_url": "https://arxiv.org/abs/1703.05175"}, "197": {"paper_id": "paper_72", "title": "Prototypical Networks for Few-shot Learning", "question_id": 8, "question": "How could we check the insight of this sentence?", "question_section": "2.4 Reinterpretation as a Linear Model", "question_trigger_sentence": "We hypothesize this is because all of the required non-linearity can be learned within the embedding function.", "question_type": "Deep/complex Question", "evidential_info": [], "composition": "[The question asks \"How could we check the insight of this sentence?\", but there is no sentence given. Hence the question can't be answered.]", "arxiv_id": "1703.05175", "s2orc_url": "https://www.semanticscholar.org/paper/c269858a7bb34e8350f2442ccf37797856ae9bca", "arxiv_url": "https://arxiv.org/abs/1703.05175"}, "198": {"paper_id": "paper_72", "title": "Prototypical Networks for Few-shot Learning", "question_id": 10, "question": "Why is the equivalence to mixture density estimation is beneficial?", "question_section": "2.6 Design Choices", "question_trigger_sentence": "We conjecture this is primarily due to cosine distance not being a Bregman divergence, and thus the equivalence to mixture density estimation discussed in Section 2.3 does not hold.", "question_type": "Deep/complex Question", "evidential_info": [], "composition": "[The paper does not mention any benefit of the equivalence.]", "arxiv_id": "1703.05175", "s2orc_url": "https://www.semanticscholar.org/paper/c269858a7bb34e8350f2442ccf37797856ae9bca", "arxiv_url": "https://arxiv.org/abs/1703.05175"}, "199": {"paper_id": "paper_72", "title": "Prototypical Networks for Few-shot Learning", "question_id": 13, "question": "Would this algorithm still work well in a 'larger' shot (e.g., 50) or even work well in a 'normal' setting?", "question_section": "3.2 miniImageNet Few-shot Classification", "question_trigger_sentence": "As can be seen in Table 2, prototypical networks achieves state-of-the-art here by a wide margin.", "question_type": "Deep/complex Question", "evidential_info": [], "composition": "[The paper does not answer whether  its approach will work well in a 'larger' shot (e.g., 50) or in a 'normal' setting.]", "arxiv_id": "1703.05175", "s2orc_url": "https://www.semanticscholar.org/paper/c269858a7bb34e8350f2442ccf37797856ae9bca", "arxiv_url": "https://arxiv.org/abs/1703.05175"}, "200": {"paper_id": "paper_72", "title": "Prototypical Networks for Few-shot Learning", "question_id": 17, "question": "How does middle crop differ from resizing?", "question_section": "3.3 CUB Zero-shot Classification", "question_trigger_sentence": "At test time we use only the middle crop of the original image.", "question_type": "Testing Question", "evidential_info": [], "composition": "[The question is self-explanatory, but the paper does not explicitly state the difference between \"middle crop\" and \"resizing\".]", "arxiv_id": "1703.05175", "s2orc_url": "https://www.semanticscholar.org/paper/c269858a7bb34e8350f2442ccf37797856ae9bca", "arxiv_url": "https://arxiv.org/abs/1703.05175"}, "201": {"paper_id": "paper_72", "title": "Prototypical Networks for Few-shot Learning", "question_id": 18, "question": "Is this sentence true? How could we check it?", "question_section": "3.3 CUB Zero-shot Classification", "question_trigger_sentence": "These attributes encode various characteristics of the bird species such as their color, shape, and feather patterns.", "question_type": "Shallow Question", "evidential_info": [], "composition": "[The question asks \"Is this sentence true? How could we check it?\", but there is no sentence given. Hence the question can't be answered.]", "arxiv_id": "1703.05175", "s2orc_url": "https://www.semanticscholar.org/paper/c269858a7bb34e8350f2442ccf37797856ae9bca", "arxiv_url": "https://arxiv.org/abs/1703.05175"}, "202": {"paper_id": "paper_73", "title": "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space", "question_id": 6, "question": "How could we define these sampling strategies as Bayesian approaches?", "question_section": "3.2 Hierarchical Point Set Feature Learning", "question_trigger_sentence": "In contrast to CNNs that scan the vector space agnostic of data distribution, our sampling strategy generates receptive fields in a data dependent manner.", "question_type": "Deep/complex Question", "evidential_info": [], "composition": "[The authors do not define any sampling strategies as Bayesian approaches in the paper.]", "arxiv_id": "1706.02413", "s2orc_url": "https://www.semanticscholar.org/paper/8674494bd7a076286b905912d26d47f7501c4046", "arxiv_url": "https://arxiv.org/abs/1706.02413"}, "203": {"paper_id": "paper_74", "title": "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation", "question_id": 8, "question": "Why an orthogonal transformation will not lose information in the input?", "question_section": "4.2. PointNet Architecture", "question_trigger_sentence": "An orthogonal transformation will not lose information in the input, thus is desired.", "question_type": "Deep/complex Question", "evidential_info": [], "composition": "[Orthogonal transformation is used in the paper because it will not lose information. However, the paper doesn't explicitly state the reason why orthogonal transformation will not lose information.]", "arxiv_id": "1612.00593", "s2orc_url": "https://www.semanticscholar.org/paper/d997beefc0922d97202789d2ac307c55c2c52fba", "arxiv_url": "https://arxiv.org/abs/1612.00593"}, "204": {"paper_id": "paper_75", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "question_id": 5, "question": "What would happen if we do not require adaptation to new tasks at test time?", "question_section": "5. Experimental Evaluation", "question_trigger_sentence": "All of the meta-learning problems that we consider require some amount of adaptation to new tasks at test-time.", "question_type": "Deep/complex Question", "evidential_info": [], "composition": "This cannot be answered because the paper itself does not address tasks that do not require adaptation at test time.", "arxiv_id": "1703.03400", "s2orc_url": "https://www.semanticscholar.org/paper/c889d6f98e6d79b89c3a6adf8a921f88fa6ba518", "arxiv_url": "https://arxiv.org/abs/1703.03400"}, "205": {"paper_id": "paper_75", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "question_id": 7, "question": "Would it be possible to regression the amplitude and phase in the case of showing only local minimum and local maximum?", "question_section": "5.1. Regression", "question_trigger_sentence": "Crucially, when the K datapoints are all in one half of the input range, the \nmodel trained with MAML can still infer the amplitude and phase in the other half of the range, demonstrating that the MAML trained model f has learned to model the periodic nature of the sine wave.", "question_type": "Deep/complex Question", "evidential_info": [], "composition": "Although the content dealing with a small number of points in the sine wave is presented, the experiment for the situation where data is given only for the local maximum and local maximum points (i.e. the point where the gradient becomes 0) is not shown.", "arxiv_id": "1703.03400", "s2orc_url": "https://www.semanticscholar.org/paper/c889d6f98e6d79b89c3a6adf8a921f88fa6ba518", "arxiv_url": "https://arxiv.org/abs/1703.03400"}, "206": {"paper_id": "paper_76", "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS", "question_id": 6, "question": "What should we do to make the model generate high quality images even with small portions of data?", "question_section": "5.1 EVALUATION ON IMAGENET", "question_trigger_sentence": "We also find that many classes on ImageNet are more difficult than others for our model; our model is more successful at generating dogs (which make up a large portion of the dataset, and are mostly distinguished by their texture) than crowds (which comprise a small portion of the dataset and have more large-scale structure).", "question_type": "Deep/complex Question", "evidential_info": [], "composition": "The information is not sufficient in paragraph provided.", "arxiv_id": "1809.11096", "s2orc_url": "https://www.semanticscholar.org/paper/22aab110058ebbd198edb1f1e7b4f69fb13c0613", "arxiv_url": "https://arxiv.org/abs/1809.11096"}, "207": {"paper_id": "paper_76", "title": "LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS", "question_id": 5, "question": "What is the relationship between interpolation and disentanglement in GAN domain?", "question_section": "5.1 EVALUATION ON IMAGENET", "question_trigger_sentence": "Our model convincingly interpolates between disparate samples, and the nearest neighbors for its samples are visually distinct, suggesting that our model does not simply memorize training data.", "question_type": "Testing Question", "evidential_info": [], "composition": "The required information is not available in the paragraphs provided.", "arxiv_id": "1809.11096", "s2orc_url": "https://www.semanticscholar.org/paper/22aab110058ebbd198edb1f1e7b4f69fb13c0613", "arxiv_url": "https://arxiv.org/abs/1809.11096"}, "208": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 2, "question": "Is the performance degrading if whole digit dataset is handcrafted? ", "question_section": "sec 7", "question_trigger_sentence": "Handcrafted set: To ensure our results do not stem from\nsimilarities between the MNIST test and training sets,\nwe also consider a handcrafted initial substitute training\nset. We handcrafted 100 samples by handwriting 10\ndigits for each class between 0 and 9 with a laptop\ntrackpad. We then adapted them to the MNIST format\nof 28x28 grayscale pixels", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The accuracy of the two substitute DNNs is reported in Figure 4 and we can see that when they use the full handcrafted dataset (100 digits) for training the max accuracy (for 6 training epochs) is degraded (67.00%) compared to the max accuracy achieved (81.20%) when the training dataset is the 150 digit samples from MNIST.", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "209": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 5, "question": "What is the performance of the proposed algorithm for the defense mechanisms that are not based on gradient masking", "question_section": "sec 8", "question_trigger_sentence": "Many potential defense mechanisms fall into a category we\ncall gradient masking. These techniques construct a model\nthat does not have useful gradients, e.g., by using a nearest\nneighbor classifier instead of a DNN", "question_type": "Deep/complex question", "evidential_info": [], "composition": "", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "210": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 8, "question": "Why are dense 1X1 convolutions computationally expensive?", "question_section": "Abstract", "question_trigger_sentence": "We notice that state-of-the-art basic architectures such as Xception [3] and ResNeXt [40] become less efficient in ex- tremely small networks because of the costly dense 1 \u00d7 1 convolutions.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "211": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 9, "question": "How pointwise group convolutions is different from 1x1 convolutions?", "question_section": "Introduction", "question_trigger_sentence": "We propose using pointwise group convolutions to reduce computation complexity of 1 \u00d7 1 convolutions.", "question_type": "Testing question", "evidential_info": [], "composition": "", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "212": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 10, "question": "What are the side effects of group convolution?\n", "question_section": "Introduction", "question_trigger_sentence": "To overcome the side effects brought by group con- volutions, we come up with a novel channel shuffle opera- tion to help the information flowing across feature channels.", "question_type": "Shallow question", "evidential_info": [], "composition": "", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "213": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 11, "question": "Is the theoretical speedup greater than the actual speedup when comparing ShuffleNet to AlexNet on real hardware?", "question_section": "Introduction", "question_trigger_sentence": "We also examine the speedup on real hardware, i.e. an off-the-shelf ARM-based computing core. The ShuffleNet model achieves \u223c13\u00d7 actual speedup (theoretical speedup is 18\u00d7) over AlexNet [21] while maintaining comparable accuracy.", "question_type": "testing question", "evidential_info": [], "composition": "", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "214": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 12, "question": "What do you mean by model pruning?", "question_section": "Introduction", "question_trigger_sentence": "Note that many existing works [16, 22, 43, 42, 38, 27] focus on pruning, compressing, or low-bit represent- ing a \u201cbasic\u201d network architecture.", "question_type": "Testing question", "evidential_info": [], "composition": "", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "215": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 13, "question": "What does the tradeoff  look like when basic network architectures are represented in low-bit computations?", "question_section": "Introduction", "question_trigger_sentence": "Note that many existing works [16, 22, 43, 42, 38, 27] focus on pruning, compressing, or low-bit represent- ing a \u201cbasic\u201d network architecture.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "216": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 14, "question": "How ShuffleNet allowed more feature maps for a given computational complexity?", "question_section": "Introduction", "question_trigger_sentence": "Compared with popular struc- tures like [30, 9, 40], for a given computation complexity budget, our ShuffleNet allows more feature map channels, which helps to encode more information and is especially critical to the performance of very small networks.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "217": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 15, "question": "Which networks introduced efficient depthwise seperable convolution into the building blocks of a state-of-the-art network?", "question_section": "Introduction", "question_trigger_sentence": "state-of-the-art networks such as Xception [3] and ResNeXt [40] introduce efficient depthwise separable convolutions or group convolutions into the building blocks to strike an excellent trade-off between representation capability and computational cost.", "question_type": "Shallow question", "evidential_info": [], "composition": "", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "218": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 16, "question": "How much percentage computation do pointwise convolutions take up in each residual unit?", "question_section": "Approach", "question_trigger_sentence": "For example, in ResNeXt [40] only 3 \u00d7 3 layers are equipped with group convolutions. As a result, for each residual unit in ResNeXt the pointwise convolutions occupy 93.4% multiplication-adds (cardinality = 32 as suggested in [40]).", "question_type": "Testing question", "evidential_info": [], "composition": "", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "219": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 17, "question": "How did the authors handle complexity constraints for their mobile networks?", "question_section": "Approach", "question_trigger_sentence": "In tiny networks, expensive pointwise convolutions result in limited number of channels to meet the complexity constraint, which might significantly damage the accuracy.\n", "question_type": "Deep/complex question", "evidential_info": [], "composition": "", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "220": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 18, "question": "Each convolution operates on that corresponding input channel group. If so, how the model learns the features from entire input space?", "question_section": "Approach", "question_trigger_sentence": "By ensuring that each con- volution operates only on the corresponding input channel group, group convolution significantly reduces computation cost.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "221": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 19, "question": "Is channel shuffle operation is similiar to that of random sparse convolution?", "question_section": "Approach", "question_trigger_sentence": "This can be efficiently and elegantly im- plemented by a channel shuffle operation (Fig 1 (c)): sup- pose a convolutional layer with g groups whose output has g \u00d7 n channels; we first reshape the output channel dimen- sion into (g, n), transposing and then flattening it back as the input of next layer. Note that the operation still takes effect even if the two convolutions have different numbers of groups.", "question_type": "testing question", "evidential_info": [], "composition": "", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "222": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 20, "question": "How channel shuffle operation works for two groups?", "question_section": "Approach", "question_trigger_sentence": "This can be efficiently and elegantly im- plemented by a channel shuffle operation (Fig 1 (c)): sup- pose a convolutional layer with g groups whose output has g \u00d7 n channels; we first reshape the output channel dimen- sion into (g, n), transposing and then flattening it back as the input of next layer. Note that the operation still takes effect even if the two convolutions have different numbers of groups.", "question_type": "Shallow question", "evidential_info": [], "composition": "", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "223": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 21, "question": "Why it is possible to say that multiple group convolutional layers works efficiently without weakening representation?", "question_section": "Approach", "question_trigger_sentence": "Channel shuffle operation makes it possible to build more powerful structures with multiple group convolutional layers.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "224": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 22, "question": "What does s in \"ShuffleNet s x\" mean? ", "question_section": "Approach", "question_trigger_sentence": "then \u201dShuffleNet s\u00d7\u201d means scaling the number of filters in ShuffleNet 1\u00d7 by s times thus overall complexity will be roughly s2 times of ShuffleNet 1\u00d7.", "question_type": "testing question", "evidential_info": [], "composition": "", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "225": {"paper_id": "paper_77", "title": "Practical Black-Box Attacks against Machine Learning", "question_id": 7, "question": "Why does the author use transmitted near-infrared imaging and not other methods?", "question_section": "Section 8", "question_trigger_sentence": "Using near-infrared (NIR) light with 700\u20131200 nm wavelength, transillumination\nimages of small animals and thin parts of a human body such as a hand or foot can be obtained. They are two-dimensional (2D) images of internal absorbing structures in a turbid medium", "question_type": "Shallow question", "evidential_info": [], "composition": "", "arxiv_id": "1602.02697", "s2orc_url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "arxiv_url": "https://arxiv.org/abs/1602.02697"}, "226": {"paper_id": "paper_83", "title": "YOLO9000: Better, Faster, Stronger", "question_id": 8, "question": "Is \"Cluster SSE\" referring to k-means clustering with standard euclidean distance?", "question_section": "Better", "question_trigger_sentence": "Table 1:Average IOU of boxes to closest priors on VOC 2007.", "question_type": "Shallow question", "evidential_info": [], "composition": "It is not clear in the paper that   \"Cluster SSE\" referring to k-means clustering with standard euclidean distance", "arxiv_id": "1612.08242", "s2orc_url": "https://www.semanticscholar.org/paper/7d39d69b23424446f0400ef603b2e3e22d0309d6", "arxiv_url": "https://arxiv.org/abs/1612.08242"}, "227": {"paper_id": "paper_83", "title": "YOLO9000: Better, Faster, Stronger", "question_id": 11, "question": "What does the dashed line in Figure 4 represent?", "question_section": "Better", "question_trigger_sentence": "Figure 4: Accuracy and speed on VOC 2007.", "question_type": "Shallow question", "evidential_info": [], "composition": "This line just shows the boundary of 30 FPS for methods faster and slower than 30FPS.", "arxiv_id": "1612.08242", "s2orc_url": "https://www.semanticscholar.org/paper/7d39d69b23424446f0400ef603b2e3e22d0309d6", "arxiv_url": "https://arxiv.org/abs/1612.08242"}, "228": {"paper_id": "paper_84", "title": "Unsupervised Learning of Depth and Ego-Motion from Video", "question_id": 16, "question": "What do \"Abs Rel\" and \"Sq Rel\" in Table 1 stand for?", "question_section": "Experiments", "question_trigger_sentence": "Table 1: Single-view depth results on the KITTI dataset [15] using the split of Eigen et al. [7] (Baseline numbers taken from [16]).", "question_type": "Testing question", "evidential_info": [], "composition": "These are error metrics and their abbreviation is not written in paper.", "arxiv_id": "1704.07813", "s2orc_url": "https://www.semanticscholar.org/paper/3abf64d10a5d9a426d864bcfd68daed370d6904c", "arxiv_url": "https://arxiv.org/abs/1704.07813"}, "229": {"paper_id": "paper_86", "title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "question_id": 7, "question": "What were the units used to represent the 3D translation vector of the input vector?", "question_section": "Grasping with Convolutional Networks and Continuous Servoing", "question_trigger_sentence": "After the 5th layer, we provide the vector vt as input to the network. The vector is represented by 5 values: a 3D translation vector, and a sine-cosine encoding of the change in orientation of the gripper about the vertical axis.", "question_type": "Shallow question", "evidential_info": [], "composition": "", "arxiv_id": "1603.02199", "s2orc_url": "https://www.semanticscholar.org/paper/494e2d5b40dcebde349f9872c7317e5003f9c5d2", "arxiv_url": "https://arxiv.org/abs/1603.02199"}, "230": {"paper_id": "paper_86", "title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "question_id": 9, "question": "What are the seven degrees of freedom for the robot arm?", "question_section": "Abstract", "question_trigger_sentence": "Our robotic manipulator platform consists of a lightweight 7 degree of freedom arm, a compliant, underactuated, two-finger gripper, and a camera mounted behind the arm looking over the shoulder.", "question_type": "Shallow question", "evidential_info": [], "composition": "", "arxiv_id": "1603.02199", "s2orc_url": "https://www.semanticscholar.org/paper/494e2d5b40dcebde349f9872c7317e5003f9c5d2", "arxiv_url": "https://arxiv.org/abs/1603.02199"}, "231": {"paper_id": "paper_86", "title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "question_id": 5, "question": "What is the set of possible values that the labels can take?", "question_section": "Grasping with Convolutional Networks and Continuous Servoing", "question_trigger_sentence": "Data for training the CNN grasp predictor is obtained by attempting grasps using real physical robots. Each grasp consists of T time steps. At each time step, the robot records the current image and the current pose, and then chooses a direction along which to move the gripper. At the final time step T, the robot closes the gripper and evaluates the success of the grasp (as described in Appendix B), producing a label.", "question_type": "Testing question", "evidential_info": [], "composition": "", "arxiv_id": "1603.02199", "s2orc_url": "https://www.semanticscholar.org/paper/494e2d5b40dcebde349f9872c7317e5003f9c5d2", "arxiv_url": "https://arxiv.org/abs/1603.02199"}, "232": {"paper_id": "paper_88", "title": "Benchmarking Deep Reinforcement Learning for Continuous Control", "question_id": 2, "question": "Why are the returns of the initial iterations included in the performance metric? Shouldn't the performance post-training be the most important metric?", "question_section": "Experiment Setup", "question_trigger_sentence": "Performance Metrics: For each report unit (a particular algorithm running on a particular task), we define its performance as 1 / sigma[from i=1 to I] * sigma[from i=1 to I]sigma[from n=1 to Ni]Rin, where I is the number of training iterations, Ni is the number of trajectories collected in the ith iteration, and Rin is the undiscounted return for the nth trajectory of the ith iteration.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "", "arxiv_id": "1604.06778", "s2orc_url": "https://www.semanticscholar.org/paper/1464776f20e2bccb6182f183b5ff2e15b0ae5e56", "arxiv_url": "https://arxiv.org/abs/1604.06778"}, "233": {"paper_id": "paper_88", "title": "Benchmarking Deep Reinforcement Learning for Continuous Control", "question_id": 8, "question": "In what \"initial settings\" can REPS perform on par with others?", "question_section": "Results and Discussion", "question_trigger_sentence": "REPS: Our main observation is that REPS is especially prone to early convergence to local optima in case of continuous states and actions. Its final outcome is greatly affected by the performance of the initial policy, an observation that is consistent with the original work of Peters et al. (2010). This leads to a bad performance on average, although under particular initial settings the algorithm can perform on par with others.", "question_type": "Testing question", "evidential_info": [], "composition": "", "arxiv_id": "1604.06778", "s2orc_url": "https://www.semanticscholar.org/paper/1464776f20e2bccb6182f183b5ff2e15b0ae5e56", "arxiv_url": "https://arxiv.org/abs/1604.06778"}, "234": {"paper_id": "paper_9", "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion", "question_id": 2, "question": "Would results improve with more images? Why or why not?", "question_section": "Abstract", "question_trigger_sentence": "-", "question_type": "shallow question", "evidential_info": [], "composition": "The paper does not contain information to judge whether results would improve or not if more images were available.", "arxiv_id": "2208.01618", "s2orc_url": "https://www.semanticscholar.org/paper/5406129d9d7d00dc310671c43597101b0ee93629", "arxiv_url": "https://arxiv.org/abs/2208.01618"}, "235": {"paper_id": "paper_9", "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion", "question_id": 7, "question": "Why do approaches that train transsofmration modules face difficulties in accessing prior knowledge with new concepts?", "question_section": "Introduction", "question_trigger_sentence": "However, these approaches are still prone to forgetting prior knowledge, or face difficulties in accessing it concurrently with newly learned concepts", "question_type": "shallow question", "evidential_info": [], "composition": "This paper does not explain why transformation modules face challenges in accessing prior knowledge and new concepts concurrently.", "arxiv_id": "2208.01618", "s2orc_url": "https://www.semanticscholar.org/paper/5406129d9d7d00dc310671c43597101b0ee93629", "arxiv_url": "https://arxiv.org/abs/2208.01618"}, "236": {"paper_id": "paper_9", "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion", "question_id": 4, "question": "What is the difference between this work and Dreambooth?", "question_section": "Abstract", "question_trigger_sentence": "-", "question_type": "testing question", "evidential_info": [], "composition": "The answer to this question, of how the current work differs from Dreambooth, cannot be answered from the content of this paper alone.", "arxiv_id": "2208.01618", "s2orc_url": "https://www.semanticscholar.org/paper/5406129d9d7d00dc310671c43597101b0ee93629", "arxiv_url": "https://arxiv.org/abs/2208.01618"}, "237": {"paper_id": "paper_9", "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion", "question_id": 13, "question": "How does the encoder approach for gan inversion work?", "question_section": "Related Work", "question_trigger_sentence": "-", "question_type": "Deep/complex question", "evidential_info": [], "composition": "This paper does not explain how encoder approaches for GAN-based image inversions work. The existence of such approaches is mentioned, but their working mechanism is not explained or discussed.", "arxiv_id": "2208.01618", "s2orc_url": "https://www.semanticscholar.org/paper/5406129d9d7d00dc310671c43597101b0ee93629", "arxiv_url": "https://arxiv.org/abs/2208.01618"}, "238": {"paper_id": "paper_9", "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion", "question_id": 8, "question": "With this technique, can the model learn an infinite number of new concepts by training them separately and merge them together? Or should they be trained together?", "question_section": "Introduction", "question_trigger_sentence": "One can therefore ask for \u201ca photograph of S\u2217 on the beach\u201d, \u201can oil painting of a S\u2217 hanging on the wall\u201d, or even compose two concepts, such as \u201ca drawing of S\u22171 in the style of S\u22172\u201d.", "question_type": "Deep/complex question", "evidential_info": [], "composition": "The authors do not analyze the upper bound on how many new concepts can be learnt at once using their proposed approach. This question is unclear and cannot be answered through a reading of this paper alone.", "arxiv_id": "2208.01618", "s2orc_url": "https://www.semanticscholar.org/paper/5406129d9d7d00dc310671c43597101b0ee93629", "arxiv_url": "https://arxiv.org/abs/2208.01618"}, "239": {"paper_id": "paper_94", "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning", "question_id": 6, "question": "Did the earlier versions of Inception (v1,v2,v3) have similiar functionality like reduction blocks?", "question_section": "Model", "question_trigger_sentence": "For the residual versions of the Inception networks, we use cheaper Inception blocks than the original Inception. Each Inception block is followed by filter-expansion layer (1 \u00d7 1 convolution without activation) which is used for scaling up the dimensionality of the filter bank before the addition to match the depth of the input. This is needed to compensate for the dimensionality reduction induced by the Inception block", "question_type": "Shallow question", "evidential_info": [], "composition": "From the various figures in the paper, it is clear that Inception v4 and Inception-ResNet models have reduction blocks/modules. But whether the earlier versions of Inception (v1,v2,v3) have these blocks cannot be answered in this paper.", "arxiv_id": "1602.07261", "s2orc_url": "https://www.semanticscholar.org/paper/b5c26ab8767d046cb6e32d959fdf726aee89bb62", "arxiv_url": "https://arxiv.org/abs/1602.07261"}, "240": {"paper_id": "paper_95", "title": "Focal Loss for Dense Object Detection", "question_id": 10, "question": "Can this technique be also used for handling misclassification in other unbalanced datasets such as time series/language models?", "question_section": "Method/3", "question_trigger_sentence": "The Focal Loss is designed to address the one-stage ob- ject detection scenario in which there is an extreme im- balance between foreground and background classes during training (e.g., 1:1000).", "question_type": "Deep/complex question", "evidential_info": [], "composition": "There is no evidence available in the paper to answer this question. Also, it is not clear from the question which \"technique\" is being asked about.", "arxiv_id": "1708.02002", "s2orc_url": "https://www.semanticscholar.org/paper/1a857da1a8ce47b2aa185b91b5cb215ddef24de7", "arxiv_url": "https://arxiv.org/abs/1708.02002"}, "241": {"paper_id": "paper_97", "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation", "question_id": 3, "question": "What are some different ways of subject driven generation besides dreambooth?", "question_section": "Preliminaries", "question_trigger_sentence": "Compositing Objects into Scenes Synthesizing a given subject in different contexts is a challenging task previously tackled in various forms and under different assumptions. ", "question_type": "Deep/complex question", "evidential_info": [], "composition": "Authors\u2019 didn\u2019t discuss any subject driven generation in this evidence paragraphs", "arxiv_id": "2208.12242", "s2orc_url": "https://www.semanticscholar.org/paper/5b19bf6c3f4b25cac96362c98b930cf4b37f6744", "arxiv_url": "https://arxiv.org/abs/2208.12242"}, "242": {"paper_id": "paper_98", "title": "Diffusion Models Beat GANs on Image Synthesis", "question_id": 1, "question": "What is Classifier Guidance?", "question_section": "Introduction", "question_trigger_sentence": "Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\u00d7256 and 3.85 on ImageNet 512\u00d7512.", "question_type": "Testing question", "evidential_info": [], "composition": "The Question is ambiguous.", "arxiv_id": "2105.05233", "s2orc_url": "https://www.semanticscholar.org/paper/64ea8f180d0682e6c18d1eb688afdb2027c02794", "arxiv_url": "https://arxiv.org/abs/2105.05233"}, "243": {"paper_id": "paper_99", "title": "Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation", "question_id": 18, "question": "What are examples of high-quality questions does not result in better retrieval results?", "question_section": "6.3 Generation vs Retrieval Quality", "question_trigger_sentence": "We can see that larger generation models lead to improved generators. However, there is little difference in retrieval metrics, suggesting that large domain targeted data is the more important criteria.", "question_type": "Testing question", "evidential_info": [], "composition": "The examples of high quality questions resulting in better retrieval cannot be answered in this paper.", "arxiv_id": "2004.14503", "s2orc_url": "https://www.semanticscholar.org/paper/ba039619ed7cd69c6b0809d5f6e442841bc299b3", "arxiv_url": "https://arxiv.org/abs/2004.14503"}}